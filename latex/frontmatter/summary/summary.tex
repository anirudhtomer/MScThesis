% !TEX root =  ../../thesis.tex

\chapter{Summary}
\label{ch : summary}

In this master thesis we fitted a finite mixture distribution for the random effects in a Bayesian linear mixed model. Our goal was to evaluate the efficacy of Bayesian model selection methods, namely Deviance information criteria, marginal likelihood and posterior predictive checks for selection of the model with the right number of components in the aforementioned mixture. For this we generated multiple artificial data sets with different types of Gaussian mixture of random effects and applied the model selection criteria on them. Since mixture models are missing data models, we implemented various definitions of DIC as given by \citet{celeux_deviance_2006}. We found that conditional data DIC's which are usually reported by software such as JAGS are not reliable for selecting the number of mixture components. DIC 4 (section \ref{eq : DIC4}) which was based on complete data likelihood performed the best among all of the DIC's. The next best performing DIC was DIC 3(section \ref{eq : DIC3}) based on marginal data likelihood. We recommend using these DIC's along with posterior predictive checks (PPC) which also worked very well to detect overfitting. We found that if inverse gamma priors were used for variance components, and uniform distribution for correlation in the distribution of random effects, then PPC's based on such models gave more extreme results in presence of overfitting the number of mixture components. We also calculated marginal likelihood for the various models using the approximation given by \citet{chib_marginal_1995} and found that it was not reliable for deciding the number of components required in the mixture of random effects.\\

While doing MCMC simulations we found some other interesting results as well, which although were not of primary interest, but are still worth mentioning here. We found that a Wishart prior for precision matrix(inverse of covariance matrix) of mixture components lead to posteriors which overestimated the precision when within subject variance was greater than between subject variance. Further we also found that if mixture components are not well separated and the number of subjects are small, then using a Dirichlet prior with small values of hyperparameter ($\leq 1$) for the weight distribution of mixture components can lead to the choice of underfitted models.