% !TEX root =  ../../thesis.tex

\chapter{Summary}
\label{ch : summary}

In this master thesis we fitted a finite mixture distribution for the random effects in a Bayesian linear mixed model. A mixture distribution for random effects allows modeling the heterogeneity introduced by ignoring certain covariates in the mean structure of the model or to take into account the non normality of random effects. We generated multiple artificial data sets with different types of Gaussian mixture of random effects and used them for testing the effectiveness of Bayesian model selection criteria (DIC, Bayes Factor, PPC) for choosing the number of component densities in the mixture distribution of random effects. Since mixture models are missing data models, we implemented various definitions of DIC as given by \citet{celeux_deviance_2006} for such models. We found that conditional data DIC's which are usually reported by software such as JAGS are not reliable for selecting the number of mixture components. DIC 4 (section \ref{eq : DIC4}) which was based on complete data likelihood performed the best among all of the DIC's. The next best performing DIC was DIC 3(section \ref{eq : DIC3}) based on marginal data likelihood. We recommend using these DIC's along with posterior predictive checks (PPC) which also worked very well to detect overfitting. We found that if inverse gamma priors were used for variance components, and uniform distribution for correlation in the distribution of random effects, then PPC's based on such models give more extreme results in presence of overfitting the number of mixture components. We also calculated marginal likelihood for the various models using the approximation given by \citet{chib_marginal_1995} and found that it was not reliable for deciding the number of components required in the mixture of random effects.\\

While doing MCMC simulations we found some other interesting results as well, which although were not of primary interest, but are still worth mentioning here. We found that a Wishart prior for precision matrix(inverse of covariance matrix) of mixture components leads to posteriors which overestimate the precision when within subject variance is greater than between subject variance. Thus, it could be a good idea to decrease scale of the intercept and the covariate corresponding to random slope, so that the corresponding variances increase in magnitude. Further we also found that if mixture components are not well separated and the number of subjects are small, then using a Dirichlet prior with small values of hyperparameter ($\leq 1$) for the weight distribution of mixture components can lead to choosing underfitted models.