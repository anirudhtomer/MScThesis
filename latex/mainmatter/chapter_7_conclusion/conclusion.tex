% !TEX root =  ../../thesis.tex

\chapter{Conclusion}
\label{ch : conclusion}

As part of this thesis we evaluated the efficacy of DIC, Marginal likelihood and Posterior predictive checks for selecting the right number of components in the mixture distribution of random effects for a Bayesian heterogeneity model. We first gave the various definitions of DIC based on observed data likelihood, complete data likelihood and conditional data likelihood. We found that $\text{DIC}_4$ is the most reliable among the various definitions of the DIC's, which matches the findings of \citet{celeux_deviance_2006}. The pattern we observed was that $\text{DIC}_4$ decreases by a large margin till the right number of components are fitted and then remains almost the same for overfitted models. However if the components are fused, or if the number of subjects are less one will see difference in DIC till right number of components is moderately large. $\text{DIC}_3$ performs similarly only when the components are well separated. \citet{celeux_deviance_2006} had also found $\text{DIC}_3$ to be the second most reliable DIC criteria. Lastly, we confirmed the suggestion of \citet{fruhwirth-schnatter_finite_2013} that that in cases where components were not well separated and subjects were less, one may have to use a dirichlet prior with slightly large values for the hyperparameter to avoid non identifiability due to empy components. Applying DIC otherwise can lead to choosing less components than there are in reality.\\

Secondly, we found that Bayes Factor via Chib's approximation was not a reliable method to detect the number of components. It however still showed signs of rightly choosing the model with only 1 component in the mixture. We also found that one has to put constraints on the ordering of the components in the MCMC simulations, otherwise while doing chib's approximation one may get further MCMC chains with a different ordering of components. This can result into very large Bayes factor estimates.\\

Lastly, for posterior predictive checks we found that overfitting can be detected easily by exploiting non identifiability due to empty components. Empty components have posterior estimates sampled from the priors, thus giving estimates which do not support the data. However because these components have very small weights, a test statistic using information from all other components also supports the data at hand to some extent. One can still see a heavy tailed skewed distribution for test statistic though. If one doesn't follow the mixed predictive approach by \citet{marshall_approximate_2003} then it can be difficult to detect overfitting/underfitting as full bayes estimates of the random effects are support the data well even when underfitting/overfitting is present. Even with the mixed predictive approach it is difficult to detect underfitting as all models give more or less the same fit to the data. Having said that in case the number of components are fitted correctly, but the posterior estimates are poor then the poor model fit can still be detected via PPC as we saw in the case of using uniform prior for correlation and indepdent gamma priors for precision of precision matrix of components.