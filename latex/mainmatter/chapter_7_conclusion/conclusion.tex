% !TEX root =  ../../thesis.tex

\chapter{Conclusion}
\label{ch : conclusion}

As part of this thesis we evaluated the efficacy of DIC, marginal likelihood and posterior predictive checks for selecting the right number of components in the mixture distribution of random effects for a Bayesian heterogeneity model. We generated artificial data sets with the response generated from a longitudinal model having a mixture distributed random component with random intercept and random slope. The various artificial data sets differed in number of mixture components for random effects, number of subjects, statistical power to detect the fixed effects, separation of mixture components and number of subjects per component. We then extended the various definitions of DIC based on observed data likelihood, complete data likelihood and conditional data likelihood, given by \citet{celeux_deviance_2006}. After testing the DIC's on the artificial data sets, we found that $\text{DIC}_4$ which was a complete DIC was the most reliable among all of the DIC's. This result also matched the findings of \citet{celeux_deviance_2006}. We also found that when the components were not well separated or when the number of subjects were less, then $\text{DIC}_1$, $\text{DIC}_2$ and $\text{DIC}_3$ did not discern among the various models much and at times also lead to the choice of underfitted models. However, they can also be used alongside $\text{DIC}_4$ when the components are well separated.\\

We also confirmed the suggestion of \citet{fruhwirth-schnatter_finite_2013} that in cases where components are not well separated, to avoid sampling of empty components one may have to use a Dirichlet prior with slightly large values for the hyperparameter. In this regard we also found that when the number of components were overfitting the mixture, some of the chains for component density parameters did not converge and simultaneously during certain iterations the component density parameters were sampled from empty components. To resolve this issue we suggest incrementally increasing the values of the hyperparameter till chains show no sign of non convergence. However care must be taken to make sure that a very large hyperparameter value is not used because in that case the prior for weight distribution does not remain non informative.\\

Secondly, we found that marginal likelihood computed using Chib's approximation is not a reliable method to detect the number of components even when the chains corresponding to posterior distribution of parameters show no sign of non convergence. We found that to do Chib's approximation, it is absolutely necessary to use ordering constraints on the component parameters in the MCMC simulations. This because Chib's approximation requires running further MCMC chains and it is possible to obtain chains with a different ordering of components each time if ordering constraints are not applied. We would also like to mention that when we used a uniform prior for correlation between random intercept and slope, then we could not use it to in Chib's approximation as the posterior distribution was not available as a well known density.\\

For posterior predictive checks we employed the mixed predictive check technique suggested by \citet{marshall_approximate_2003} and found that overfitting models can be detected easily by exploiting non identifiability due to empty components. Empty components have posterior estimates sampled from the priors, thus giving estimates which do not support the data. More specifically we obtained right skewed distribution for the test statistic (equation \ref{eq : ppc_test_statistic}) in case of the overfitted models. However because the nearly empty components have very small posterior weights, this posterior predictive check requires a very large sample size. We also found that it was difficult to detect models with less number of components in the mixture because such models also gave a very good fit to the data. Having said that, in case the number of components are fitted correctly, but the posterior estimates are poor then the poor model fit can still be detected via PPC. This was especially observed in the case of using uniform prior for correlation and inverse gamma priors for random component variances, because the random component variances were underestimated. An interesting observation in this regard was that in case the actual random component variances were smaller than the variance of error component, then using an inverse Wishart prior for the random component variances leads to posteriors with underestimated variances.\\

Lastly, we applied the Bayesian heterogeneity model to the blood donor data set and found a mixture of 2 components to be ideal for the distribution of random effects. This was in contrast with the number of components proposed by \citet{nasserinejad_prevalence_2015} using growth mixture models. It is however interesting to note that the components we found were a subset of the components proposed by \citet{nasserinejad_prevalence_2015}. More specifically, the two components that we observed differed in a way such that the subgroup of subjects with higher baseline Hemoglobin level also had a rapid decrease in Hemoglobin level with number of donations.