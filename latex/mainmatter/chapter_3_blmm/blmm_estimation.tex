% !TEX root =  ../../thesis.tex

\section{Estimation of parameters in the Bayesian heterogeneity model}
In this section we will discuss some of the challenges in Bayesian estimation of parameters in the Bayesian heterogeneity model. We will also discuss the approaches we used to deal with them in this thesis.

\subsection{Marginal vs. Hierarchical model}
Suppose that in our heterogeneity model we know the alloation vector $S_i$ for each subject. Then conditional on knowing $S_i=k$ the following LMM equation has a hierarchical interpretation.

$$\boldsymbol{y}_i|\boldsymbol{b}_{i} \sim N(\boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_{i},\boldsymbol{\varepsilon}_{i}), 
\boldsymbol{\varepsilon}_{i} \sim N_{m_i}(\boldsymbol{0}, R_i)$$

One can however integrate out the random effects $\boldsymbol{b}_{i}$ and obtain the corresponding marginal Bayesian heterogeneity model,

$$\boldsymbol{y}_i \sim N(\boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_k^C, \boldsymbol{\varepsilon}_{i}^*), 
\boldsymbol{\varepsilon}_{i} \sim N_{m_i}(\boldsymbol{0}, \boldsymbol{Z}_{i}G_k\boldsymbol{Z}_{i}^T+ R_i)$$

The marginal model is recommended by \citet{fruhwirth-schnatter_bayesian_2004} for good mixing of chains, and while doing the simulation study(presented in chapter \ref{ch : simulation_study}) we found that claim to be true. However the Marginal model took quite a long time to for each iteration. Secondly it did not give posterior estimates of the random effects $\boldsymbol{b}_i$ which were required for calculation of certain definitions of DIC (discussed in chapter \ref{ch : model_selection}). Besides we found that a model with hierarchical centering had as much autocorrelated estimates as the marginal model and took less time for each iteration.

\subsection{Hierarchical centering}
The random effects $\boldsymbol{b}_i$ in a mixed model could be seen as random deviations from the fixed effects$(\boldsymbol{\beta})$ with a mean $\boldsymbol{0}$. For a longitudinal data set, it means that the overall effect of a covariate like time for a subject should be the sum of both fixed and random effects. In this case matrices $\boldsymbol{X}$ and $\boldsymbol{Z}$ both share columns corresponding to the variable time. To enforce the mean $\boldsymbol{0}$ on the random effects in a mixture distribution the following condition should be satisfied.

\begin{equation}
\label{eq : non_centred_constraint}
E(\boldsymbol{b}_i | \boldsymbol{\phi}) = \sum_{k=1}^{K} \eta_k N_q(\boldsymbol{b}_k^C, G_k) = 0
\end{equation}

where $\boldsymbol{\phi}$ is the vector $(\boldsymbol{b}_1^C, \boldsymbol{b}_2^C, \ldots, \boldsymbol{b}_K^C, \boldsymbol{\eta}, G_1, G_2, \ldots, G_K)$. This further means that $E(\boldsymbol{y}_i | \boldsymbol{\phi}) = \boldsymbol{X}_{i}\boldsymbol{\beta}$. This parametrization, which was also used in the original paper on Heterogeneity model \citep{verbeke_linear_1996} is called noncentralized parametrization. The centralized parametrization assumes that the random effects are not deviations from the fixed effects and are centred around a non zero mean.
The choice of parametrization has an effect on the rate of convergence while estimating parameters using MCMC. While doing the simulation study we figured out that imposing the constraint in equation \ref{eq : non_centred_constraint} drastically slows the convergence as well increases the autocorrelation in parameter estimates. Thus, in this thesis we have only used hierarchically centred parametrization.

\subsection{Choice of priors}
\label{subsec : choice_priors}
Since we are following a Bayesian paradigm parameters in the Bayesian heterogeneity model are random variables and thus need to have a prior distribution. There are certain difficulties in specifying the prior though, especially that it can be difficult to implement theoretically preferred prior distributions. As an example we will begin with the choice of prior for the mean $(b_k^C)$ and covariance matrix $(G_k)$ of components densities in the mixture distribution of random effects. We know that both $(b_k^C)$ and $(G_k)$ are unknown, and hence to obtain the joint posterior in closed form one has to specify the conditionally conjugate prior $b_k^C | G_k \sim N(\boldsymbol{\mu}_0, \frac {G_K} {N_0})$ and $G_k^{-1} \sim \mathcal{W} (n_0, \Psi)$. The marginal prior of $b_k^C$ is a multivariate T distribution. However the problem with this approach is that it is firstly difficult to implement in JAGS and even if one does, the extra computationally intensive procedure does not provide much advantage in practice. It is thus a widespread practice to use independent priors for mean and covariance matrix \citep[chap. 17]{gelman_data_2006}. For e.g. a common non informative prior for $b_k^C$ (say, having only random intercept and slope) is $N(\boldsymbol{0}, \begin{bmatrix}10^5 & 0 \\ 0 & 10^5\end{bmatrix})$. This prior is equivalent to specifying indepedent diffuse univariate normal priors for random intercept mean and random slope mean.

\subsubsection{Choice of prior for covariance matrix}
The choice of prior for the covariance matrix is an interesting one. \citet[pg. 260]{lesaffre_bayesian_2012} suggest using an inverse wishart prior with small diagonal elements for the scale hyperparameter and degrees of freedom hyperparameter equal to the dimension of parameter at hand. For e.g $IW(\begin{bmatrix}0.01 & 0 \\ 0 & 0.01\end{bmatrix}, 2)$ could be one such prior. For precision matrix one can use the Wishart prior $W(\begin{bmatrix}10 & 0 \\ 0 & 10\end{bmatrix}, 2)$. i.e. the scale is inverse of the scale hyperparameter for inverse wishart distribution. As we found later in our simulations, a big value for diagnoal elements of scale matrix of Wishart distribution influence the posterior more than the likelihood does.\\

One can also choose indepdent gamma priors for random intercept and random slope and uniform prior $U(-1,1)$ for correlation between the two. The upside of this approach is that it gives almost the same estimates as one can get from frequentist analysis, but the downside is that MCMC iterations are slow as the posterior is not available as a known density. Another benefit of this approach, as we later found out during simulations is that when more components than needed are fitted to the data, then the extra components tend to have very high variance estimates for random intercept and random slope. This property can be used to make decisive posterior predictive checks.

\subsubsection{Choice of priors for $\boldsymbol{\beta}$ and $\sigma^2$}
We assume that the parameters $\boldsymbol{\beta}$ and $\sigma^2$ are indepdent from $\boldsymbol{b}_1^C, \boldsymbol{b}_2^C, ..., \boldsymbol{b}_K^C, G_1, G_2, ..., G_K$. The problem of choosing a conjugate prior such that the joint posterior of $(\boldsymbol{\beta}, \sigma^2)$ is available as a known density, is similar to the problem we discussed in previous two subsections. It is quite common in practice to use indepdent univariate normal priors such as $N(0, 10000)$ for each of the $\beta_p$ and a $\text{Gamma}(0.0001, 0.0001)$ prior for $\tau = \frac 1 {\sigma^2}$ \citep[chap. 17]{gelman_data_2006}.

\subsection{Label Switching}
We use a mixture distribution for random effects in the Bayesian heterogeneity model. However we do not know the allocation vector $\boldsymbol{S}$ in advance. In this case the mixture likelihood is given by equation \ref{eq : obs_data_likelihood}. This likelihood function is symmetrical and has $K!$ modes \citep[pg. 44]{fruhwirth-schnatter_finite_2013}. This creates a problem called label switching while doing the MCMC procedure.\\

To idea of label switching could be explained with this simple example. Suppose we have a mixture distribution $0.5N(5,1) + 0.5N(7,1)$of two components $C_1$ and $C_2$ and we sampled a few observations from it. Using the MCMC procedure we will estimate the parameters of the two components. The MCMC procedure for missing data models like mixture models uses a technique called data augmentation. The idea of data augmentation is similar to the frequentist EM algorithm. ie. we begin with some random allocation vector $\boldsymbol{S}^0$ and estimate parameters using complete data likelihood. An example of a complete data likelihood for Bayesian heterogeneity model is expression \ref{eq : complete_data_likelihood}. For the MCMC sampler labels $\mu_1$ and $\mu_2$ exist for the two means, however any of the two can correspond to $\mu_{C1}$ or $\mu_{C2}$. i.e. Labels are not associated with actual components. Assume that the allocation vector we began with assigns all observations from component $C_1$ under label 1 and all observations from component $C_2$ under label 2. Under such a scheme $(\mu_1,\mu_2) = (5,7)$ is likely. However if we take a conjugate of this allocation vector $(\mu_1,\mu_2) = (7,5)$ will also be accepted. This because we have a mixture likelihood function which is bimodal.\\

Now let us imagine a scenario where because of our initial allocation vector, parameter estimates are $(\mu_1, \mu_2) = (5.5,6.5)$. So far it seems $\mu_1$ represents $\mu_{C1}$ and $\mu_2$ represents $\mu_{C2}$. Now we estimate allocation vector conditional on these estimates in MCMC. Supposing that an observation with value 6.5 originally from component $C_2$ gets allocated to component $C_1$ and similarly an observation with value 5.5 from component $C_1$ gets allocated to $C_2$. Unless we impose some constraint like $\mu_1 < \mu_2$, under the current situations even $(\mu_1,\mu_2) = (6.5, 5.5)$ could be sampled by MCMC. If the sampler keeps on arbitrarily switching between the two equivalent posterior regions then because of this label switching, one may obtain a bimodal posterior for both $\mu_1$ and $\mu_2$. Thus both posteriors will be partially explored and any inference based on this posterior will be useless.

\subsubsection{Dealing with label switching}
One of the techniques we used for dealing with label switching was imposing an identifiability constraint such as $\mu_1 < \mu_2$. The difficulty with this approach is that it is easier to do in univariate mixtures but not with multivariate mixtures. For e.g. a multivariate mixture that we have is mixture distribution for joint distribution of random intercept and random slope. In this thesis we employ identifiability constraint only on either the random intercept or the random slope depending upon the variance of each random effect. It is interesting to note that if more components than needed are chosen than label swtiching is unavoidable, and should also be seen as an indicator for overfitting \citep[pg. 104]{fruhwirth-schnatter_finite_2013}.\\

Postprocessing of MCMC chains by relabeling with output has also been suggested in the literature \citep{richardson_bayesian_1997,stephens_dealing_2000}. We also employed this method in the calculation of Bayes Factor, as that also involves running further MCMC chains (expression \ref{eq : rao_blackwellization_wishart}). Without careful relabeling of output one can obtain a Bayes factor $\to 0$ and thus reject the model outright.

\section{Mixture model identifiability: Equal or empty components... DIrichlet prior}
A mixture model will also be unidentified if we have an empty component or two components with the same parameters. Suppose the true number of components is $K^{true}$ and we fit $K = K^{true} + 1$ components. In the MCMC sampler suppose one of the components is assigned any observation. Thus the posterior for the parameters will remain the same as the prior. Assuming that the prior for weight distribution $\boldsymbol{\eta}$was a Dirichlet prior $\mathcal{D}(0.5, 0.5, \ldots, 0.5)$, then we will have a posterior for $\boldsymbol{\eta}$ such that the $K^{th}$ component will always have an almost 0 weight in the mixture distribution. This means that at the end of MCMC sampling the component density's posterior will be same as its prior and no observations will be allocated to the component. In such cases the true number of components could be estimated by the count of components with non zero number of allocations. However this situation could be avoided with a stronger prior on the weight distribution which forces the posterior distribution of $\boldsymbol{\eta}$ to be such that no components are empty at the end of MCMC sampling. Identification of number of components in such case is explained in the next section.\\