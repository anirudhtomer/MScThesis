% !TEX root =  ../../thesis.tex

\chapter{Bayesian linear mixed effects model}
\label{ch : blmm}

\section{Introduction to linear mixed model}
\label{sec : lmm}
A linear mixed effects model, also known as linear mixed model(LMM) is a statistical model for hierarchical data. For e.g. one such hierarchy is formed when repeated observations are taken from multiple patients and patients are grouped under multiple hospitals. The specialty of these models is that apart from the fixed effects, they also model the correlation between the observations falling in the same group at a certain level in the hierarchy. The correlation is modeled using a combination of random effects and measurement error, and the response is modeled as a linear function of both fixed and random effects.\\

There are many synonymous terminologies for data sets which are hierarchical in nature, albeit with subtle nuances differentiating them. In this thesis our focus was on longitudinal data sets. A longitudinal data set is the one where multiple observations are collected from subjects at different points in time. For e.g. measurement of hemoglobin of 20 patients with observations taken every month for a period of 24 months. The observations collected from a subject are correlated in such a case, and a linear mixed model provides a suitable framework to model that correlation.

\subsection{LMM definition}
\label{subsec : lmm_definition}
The LMM for the observations of the $i^\text{th}$ subject among the $n$ subjects can be written as

\begin{equation}
\label{eq : lmm_definition}
\boldsymbol{y}_i = \boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_{i} + \boldsymbol{\varepsilon}_{i}
\end{equation}

where $1 \le i \le n$,\\
$\boldsymbol{y}_i = {(y_{i1}, y_{i2}, \ldots, y_{im_i})}^T$ is a vector of observations for the $i^\text{th}$ subject taken at $m_i$ time points, and $\boldsymbol{y} = {(\boldsymbol{y}_1^T, \boldsymbol{y}_2^T, ..., \boldsymbol{y}_n^T)}^T$ represents the entire vector of observed responses,\\
$\boldsymbol{X}_i = {(\boldsymbol{x}_{i1}^T, \boldsymbol{x}_{i2}^T, \ldots, \boldsymbol{x}_{im_i}^T)}^T$ is the $m_i \times (d+1)$ design matrix for the $i^\text{th}$ subject in the data set, and $\boldsymbol{X} = {(\boldsymbol{X}_1, \boldsymbol{X}_2, ..., \boldsymbol{X}_n)}^T$ is the entire design matrix for the $n$ subjects,\\
$\boldsymbol{\beta} = {(\beta_0, \beta_1, \ldots, \beta_d)}^T$ is a $(d+1) \times 1$ vector of fixed effects with $\beta_0$ being the intercept,\\
$\boldsymbol{Z}_i = {(\boldsymbol{z}_{i1}^T, \boldsymbol{z}_{i2}^T, \ldots, \boldsymbol{z}_{im_i}^T)}^T$ is the $m_i \times q$ design matrix of covariates multiplying the random effects,\\
$\boldsymbol{b}_i = {(b_{0i}, b_{1i}, \ldots, b_{(q-1)i})}^T$ is a $q \times 1$ vector of random effects with $b_{0i}$ being the random intercept. The random effects $\boldsymbol{b}_i \sim N_q(\boldsymbol{0}, G)$ with $G$ being the $q \times q$ covariance matrix,\\ 
$\boldsymbol{\varepsilon}_{i} = {(\varepsilon_{i1}, \varepsilon_{i2}, \ldots, \varepsilon_{im_i})}^T$ is a $m_i \times 1$ vector of measurement errors. The errors $\boldsymbol{\varepsilon}_{i} \sim N_{m_i}(\boldsymbol{0}, R_i)$ with $R_i$ being the $(m_i \times m_i)$ covariance matrix of errors,\\

The errors $\boldsymbol{\varepsilon}_{i}$ and the random effects $\boldsymbol{b}_i$ are assumed to be independent. $R_i$ is usually a diagonal matrix of the form $\sigma^2I_{m_i}$. While one might only model the correlation between the observations of a subject using random effects, it is also possible to model the serial correlation component in which case $R_i$ is not a diagonal matrix. 

\section{Motivation for Bayesian linear mixed model}
\label{sec : blmm}
One of issues with the frequentist LMM is that while the parameters in matrices $G$ and $R_i$ are estimated using ML/REML, only a point estimate is further used in estimation of fixed effects (see \cite[chap. 5]{verbeke_linear_2009}). Hence the uncertainty in estimation of random effects is ignored. Although frequentist inference approaches try to mitigate this issue by modifying the distributional assumptions of the test statistic for fixed effects \citep[pg. 56]{verbeke_linear_2009}, a Bayesian approach takes into account the variability in parameter estimates natively. A similar problem occurs in the estimation of random effects $\boldsymbol{b}_i$. The frequentist strategy is to use Empirical Bayes estimates, where the posterior distribution of random effects use point estimates of parameters $G$ and $R_i$. Thus the uncertainty in estimation is ignored again. On the other hand the Bayesian approach averages over the entire posterior distribution of the hyperparameters to obtain the posterior $p(\boldsymbol{b}_i|\boldsymbol{y})$. In light of these reasons, we have modeled our data using Bayesian linear mixed model.\\

The Bayesian linear mixed model or BLMM can be obtained by assigning a distribution to all of the parameters involved in a LMM. It means that the model presented in section \ref{subsec : lmm_definition} can be extended by giving a prior distribution for the following:
\begin{itemize}
\item $\sigma^2 \sim p(\sigma^2)$
\item $\boldsymbol{\beta} \sim p(\boldsymbol{\beta})$
\item $G \sim p(G)$
\end{itemize}

\section{Motivation for mixture of random effects}
As we saw above, in a LMM the random effects are assumed to be multivariate normally distributed. It could be too strong an assumption though in certain cases. An example is the longitudinal studies where at a given time point we would like to categorize subjects in groups. For e.g. group with a high risk of having a certain disease in future vs. group with a low risk. In retrospective studies this task is easier because we know exactly which patients were diagnosed with the disease and which were not. However this could be difficult in a study where we would like to categorize patients into different groups well before diagnosis. Here is a toy example: Imagine that in a longitudinal study we are measuring a response $Y$ which is a continuous indicator of a disease. Assume that from a previous study it is known that patients which are in high risk group for the disease tend to have a higher response $Y$ during all times. Also assume that the trend of $Y$ over time remains the same for both groups otherwise. Figure \ref{fig : random_slope_dummy_data} shows individual profiles of such subjects from a simulated dataset. Looking at this plot we can say that a random intercept component will be enough to model individual profiles. Since we do not know which patient belongs to which group, this heterogeneity can be appropriately modeled by considering that the random intercept is a mixture of two normal components. Another reason for using a mixture distribution is that the random effects distribution may not be of a known form and the mixture distribution may very well approximate it.\\

\begin{figure}
	\centering
	\captionsetup{justification=centering}
	\includegraphics[scale=0.5]{mainmatter/chapter_3_blmm/random_slope_dummy_data.png}
	\caption{Individual profiles of 30 subjects from each group.}
	\label{fig : random_slope_dummy_data}
\end{figure}

In a LMM is quite common to use histogram of Empirical Bayes estimates of random effects to detect groups of individuals. However \citet{verbeke_linear_1996} have shown that if the prior is misspecified (for e.g. if in our example we use a univariate normal distribution), then the histogram of estimates of random effects will be shrunk towards the prior distribution. Thus it can become impossible to classify the subjects into different categories if the shrinkage is too high for the Empirical Bayes estimates of random effects. A solution to this problem is using a mixture of Gaussian components for random effects distribution. A linear mixed model with such a mixture distribution for random effects is termed as the heterogeneity model.

\subsection{Bayesian heterogeneity model}
\label{subsec : bhtge}
The formal definition of a Bayesian heterogeneity model can be given by extending the Bayesian linear mixed model definition given in section \ref{sec : blmm}. Since, now the random effects have a Gaussian mixture distribution we will use the following notation to express the distribution mathematically.

$$\boldsymbol{b}_i \sim \sum_{k=1}^{K} \eta_k N_q(\boldsymbol{b}_k^C, G_k)$$\\
where $\boldsymbol{b}_k^C$ and $G_k$ are the mean vector and covariance matrices for the $k^\text{th}$ component in the mixture distribution respectively. The vector $\boldsymbol{\eta} = (\eta_1, \eta_2, \ldots, \eta_K)^T$ is the weight distribution for the component densities. The vector $\boldsymbol{S}=(S_1, S_2, ..., S_n)^T$ represents the allocation vector for the $n$ subjects. Since we are following the Bayesian paradigm, in addition to prior distribution for $\boldsymbol{\beta}$ and $\sigma^2$ we also have prior for each of the parameters in $\boldsymbol{\nu} = (\boldsymbol{b}_1^C, \boldsymbol{b}_2^C, \ldots, \boldsymbol{b}_K^C, G_1, G_2, \ldots, G_K, \boldsymbol{\eta})$.

\section{Estimation of parameters in the Bayesian heterogeneity model}
In this section we discuss some of the challenges in Bayesian estimation of parameters in the Bayesian heterogeneity model. We also discuss the approaches we used to deal with them in this thesis.

\subsection{Marginal vs. hierarchical model}
Suppose that in the heterogeneity model the random effect $\boldsymbol{b}_i$ are known, then following LMM equation has a hierarchical interpretation.

\begin{equation}
\begin{split}
\boldsymbol{y}_i|\boldsymbol{b}_{i} &\sim N(\boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_{i},\boldsymbol{\varepsilon}_{i})\\ 
\boldsymbol{\varepsilon}_{i} &\sim N_{m_i}(\boldsymbol{0}, R_i)
\end{split}
\end{equation}

It is important to note that the distribution of $\boldsymbol{y}_i$ does not depend on allocation $S_i$ directly due to hierarchical independence. i.e. It is $\boldsymbol{b}_{i}$ which depends on $S_i$ because $\boldsymbol{b}_{i}|S_i \sim N(\boldsymbol{b}_{S_i}^C, G_{S_i})$. However if one knows the random effect then knowing the allocation is not necessary in the hierarchical model. i.e. $p(\boldsymbol{y}_i|\boldsymbol{b}_{i}, S_i) = p(\boldsymbol{y}_i|\boldsymbol{b}_{i})$. Though it is possible to integrate out the random effects $\boldsymbol{b}_{i}$ and obtain the corresponding Bayesian heterogeneity model where $\boldsymbol{y}_i$ depend on allocation $S_i$,.

\begin{equation}
\begin{split}
\boldsymbol{y}_i|S_i &\sim N(\boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_{S_i}^C, \boldsymbol{\varepsilon}_{i}^*)\\ 
\boldsymbol{\varepsilon}_{i}^* &\sim N_{m_i}(\boldsymbol{0}, \boldsymbol{Z}_{i}G_{S_i}\boldsymbol{Z}_{i}^T+ R_i)
\end{split}
\end{equation}

The marginal model is recommended by \citet{fruhwirth-schnatter_bayesian_2004} for good mixing of chains, and while doing the simulation study (presented in chapter \ref{ch : simulation_study}) we found that claim to be true. However, we observed that estimating the parameters using a marginal model took quite a long time for each iteration. It also did not give posterior estimates of the random effects $\boldsymbol{b}_i$ which were required for calculation of certain definitions of DIC (discussed in chapter \ref{ch : model_selection}). Besides we found that a model with hierarchical centering (discussed next) took less time for each iteration and had as much autocorrelation in the posterior density samples as with the use of the marginal model.

\subsection{Hierarchical centering}
The random effects $\boldsymbol{b}_i$ in a mixed model can be seen as random deviations from the fixed effects $(\boldsymbol{\beta})$ with a mean $\boldsymbol{0}$. For a longitudinal data set, it means that the overall effect of a covariate such as the intercept for a subject should be the sum of both fixed and random effects. In this case matrices $\boldsymbol{X}$ and $\boldsymbol{Z}$ both share columns corresponding to the variable intercept. To enforce the mean $\boldsymbol{0}$ on the random effects in a mixture distribution of random effects, the following condition should be satisfied.

\begin{equation}
\label{eq : non_centred_constraint}
E(\boldsymbol{b}_i | \boldsymbol{\nu}) = \sum_{k=1}^{K} \eta_k N_q(\boldsymbol{b}_k^C, G_k) = 0
\end{equation}

where $\boldsymbol{\nu}$ is defined in section \ref{subsec : bhtge}. This further means that $E(\boldsymbol{y}_i | \boldsymbol{\nu}) = \boldsymbol{X}_{i}\boldsymbol{\beta}$. This parametrization, which was also used in the original paper on heterogeneity model \citep{verbeke_linear_1996} is called the non-centralized parametrization. The centralized parametrization assumes that the random effects are centered around a non zero mean. The choice of parametrization has an effect on the rate of convergence of the chains in the MCMC process. While doing the simulation study we observed that imposing the constraint in equation \ref{eq : non_centred_constraint} drastically slowed the convergence as well increased the autocorrelation in parameter estimates. Thus, in this thesis we have only used centralized parametrization.

\subsection{Starting values}
\label{subsec : choice_starting_values}
The choice of starting values is important in mixtures especially when the components are not well separated. In the R package bayesmix \citep{gruen_bayesmix:_2015} the authors used $\frac 1 {k+1}, \frac 2 {k+1}, ..., \frac 2 {k+1}$ quantiles of the sample data for the starting values of the means $\mu_1, \mu_2, ..., \mu_K$ of the $K$ components. In the Bayesian heterogeneity model we obtained a rough estimate of the random component from the data as described in section \ref{subsec : ds_description} and then calculated the sample quantiles. We found out that it resulted into chains with an improved convergence. We also calculated starting values for the fixed effects $\boldsymbol{\beta}$ using OLS because OLS parameter estimates are unbiased and consistent \citep[pg. 50]{verbeke_linear_2009}.

\subsection{Choice of priors}
\label{subsec : choice_priors}
Since we followed the Bayesian paradigm, the parameters in the Bayesian heterogeneity model are random variables and we were required to assign a prior distribution to the parameters. For the $k^\text{th}$ component in the mixture of random effects, both the mean $\boldsymbol{b}_k^C$ and covariance matrix $G_k$ were unknown. To obtain the joint posterior of these parameters as a known density, the conditionally conjugate prior $\boldsymbol{b}_k^C | G_k \sim N(\boldsymbol{\mu}_0, \frac {G_K} {g_0})$ and $G_k^{-1} \sim \mathcal{W} (n_0, \Psi)$ was suitable. Here $\boldsymbol{\mu}_0, g_0, n_0, \Psi$ are the hyperparameters for the corresponding prior distributions. During our simulations we found that choosing $g_0=1$ yielded very high values for variance components whereas for $g_0$ > 1 JAGS was unable to find a sampler. We thus stuck to the widespread alternative practice of specifying independent priors for the mean $\boldsymbol{b}_k^C$ and covariance matrix $G_k$ \citep[chap. 17]{gelman_data_2006}. For e.g. a common non informative prior for $\boldsymbol{b}_k^C$ (say, having only random intercept and slope) is $N(\boldsymbol{0}, \begin{bmatrix}10^5 & 0 \\ 0 & 10^5\end{bmatrix})$. This prior is equivalent of specifying independent diffuse univariate normal priors for the mean of random intercept and random slope respectively.

\subsubsection{Choice of prior for covariance matrix}
The choice of prior for the covariance matrix $(G_k)$ posed an interesting problem to us. \citet[pg. 260]{lesaffre_bayesian_2012} suggest using an inverse Wishart prior with small diagonal elements for the scale hyperparameter and degrees of freedom hyperparameter equal to the dimension of $G_k$. For e.g $IW(\begin{bmatrix}0.1 & 0 \\ 0 & 0.1\end{bmatrix}, 2)$ could be one such prior. The corresponding prior for precision matrix $G_k^{-1}$ is the Wishart prior $W(\begin{bmatrix}10 & 0 \\ 0 & 10\end{bmatrix}, 2)$. i.e. the scale of Wishart prior is inverse of the scale hyperparameter for inverse Wishart distribution. As we found later in our simulations, a big value for diagonal elements of scale matrix of Wishart distribution influenced the posterior more than the likelihood did.\\

Another option is to use independent inverse gamma priors for random intercept and random slope variances and uniform prior $U(-1,1)$ for the correlation between the two random effects. The upside of this approach is that it may give almost the same estimates as one can get from frequentist analysis, but the downside is that MCMC iterations are slower because the posterior is not available as a known density. Another benefit of this approach, as we later found out during simulations is that when the mixture distribution is overfitted, then the extra components tend to have very high variance estimates for random intercept and random slope. We also used this property to design a posterior predictive check.

\subsubsection{Choice of priors for $\boldsymbol{\beta}$ and $\sigma^2$}
We assumed that the parameters $\boldsymbol{\beta}$ and $\sigma^2$ are independent from parameters in the set $\boldsymbol{\nu}$ (section \ref{subsec : bhtge}). The problem of choosing a conjugate prior for $\boldsymbol{\beta}$ and $\sigma^2$ is similar to what we discussed in the section above. The solution is also alike, i.e. using independent univariate normal priors such as $N(0, 10000)$ for each of the $\beta_d$ and a $\text{Gamma}(0.0001, 0.0001)$ prior for $\tau = \frac 1 {\sigma^2}$ \citep[chap. 17]{gelman_data_2006}.

\subsubsection{Choice of prior for $\boldsymbol{\eta}$}
The conjugate prior for the weight distribution $\boldsymbol{\eta}$ is the Dirichlet prior $Dir(a_0, a_1,..., a_K)$. \citet[pg. 105]{fruhwirth-schnatter_finite_2013} suggest choosing values of hyperparameters $a_0, a_1,..., a_K$ to be greater than 1 in data sets where one of the components is nearly empty or the components are not well separated. For such data sets we observed label switching (discussed next) when we used Dirichlet prior with hyperparameter value equal to 1 or less than 1. We found out during the simulation study that choosing slightly larger values for the hyperparameter, such as 2 or 3, indeed mitigated the issue of label switching (section \ref{subsec : dic_simulation_results}) when the mixture of random effects was underfitted or correctly fitted. For models with overfitted mixtures even very large value of the hyperparameters, such as 14 or 15 did not mitigate the issue.

\subsection{Label Switching}
\label{subsec : label_switching_blmm}
We used a mixture distribution for random effects in the Bayesian heterogeneity model and thus the allocation vector $\boldsymbol{S}$ for the mixture was not known in advance. In this case the mixture likelihood for the response $\boldsymbol{y}$ is given by equation \ref{eq : obs_data_likelihood}. The mixture likelihood function is symmetrical and has $K!$ modes \citep[pg. 44]{fruhwirth-schnatter_finite_2013}. This creates a problem called label switching while running the MCMC procedure.\\

The label switching problem can be explained with the following example. Suppose we have a mixture distribution $0.5N(5,1) + 0.5N(7,1)$ of two components $C_1$ and $C_2$ and we have few observations sampled from the mixture. Using the MCMC procedure we can one estimate the parameters of the two components. The MCMC procedure for missing data models like mixture models uses a technique called data augmentation. The idea of data augmentation is similar to the frequentist EM algorithm. i.e. we begin with some random allocation vector $\boldsymbol{S}_\text{initial}$ and estimate parameters using the complete data likelihood. An example expression of a complete data likelihood for Bayesian heterogeneity model is expression \ref{eq : complete_data_likelihood}. For the MCMC sampler, labels $\mu_1$ and $\mu_2$ exist for the two means, however either one can correspond to $\mu_{C1}$ or $\mu_{C2}$. i.e. labels are not associated with actual components from the beginning. Assume that the allocation vector $\boldsymbol{S}_\text{initial}$ is such that it assigns all observations from component $C_1$ under label 1 and all observations from component $C_2$ under label 2. Under such a scheme a posterior sample $(\mu_1,\mu_2) = (5,7)$ is likely. However if we take a conjugate of this allocation vector then $(\mu_1,\mu_2) = (7,5)$ is also likely to be sampled. This can be attributed to the fact that we have a mixture likelihood function which is bimodal. In cases where the components are not well separated, because of a certain scheme of allocations $\boldsymbol{S}$, the sampler might sample $\mu_1$ from both modal regions of the likelihood resulting into a posterior which is bimodal as well. However it can also lead to partially explored posteriors, which may not be useful for making any inference.

\subsubsection{Dealing with label switching}
One of the techniques used for dealing with label switching is imposing a formal identifiability constraint such as $\mu_1 < \mu_2$. However \citet{fruhwirth-schnatter_bayesian_2004} suggest that arbitrary identifiability constraints should not be applied as they are often ineffective. Instead in an example they chose an identifiability constraint based on pre-analysis of the various modal regions. In the case of Bayesian heterogeneity model the mean is a vector comprising of random intercept and random slope means. In this thesis we have applied identifiability constraints on the mean vector based on graphical exploration of the mixture density as shown in section \ref{subsec : ds_description}. It is important to note that if more components than needed are chosen, then label switching is unavoidable, and could also be seen as an indicator for overfitting \citep[pg. 104]{fruhwirth-schnatter_finite_2013}.\\

Another technique to deal with label switching is post-processing of MCMC chains by relabeling the output\citep{richardson_bayesian_1997,stephens_dealing_2000}. We too employed this technique while approximating marginal likelihood using Chib's approximation (section \ref{sec : marginal_likelihood}), albeit in a very simplified form.