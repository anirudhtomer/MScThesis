% !TEX root =  ../../thesis.tex

\chapter{Bayesian linear mixed effects model}
\label{ch : blmm}

\section{Introduction to linear mixed model}
\label{sec : lmm}
A linear mixed effects model, also known as linear mixed model(LMM) is a statistical model for data which is hierarchical in structure. The specialty of the models is that apart from the fixed effects, they also model the correlation between the observations falling in the same group at a certain level in the hierarchy. The correlation is modeled using the random effects and the response is modeled as a linear function of both fixed and random effects.\\

There are many synonymous terminologies for data sets which are hierarchical in nature albeit with subtle nuances differentiating them. In this thesis our focus will be on Longitudinal data sets. A longitudinal data set is the one where multiple observations are collected from subjects at different points in time. For e.g. measurement of Hemoglobin of 20 patients with observations taken every month for a period of 24 months. Since the observations collected from a subject will be correlated a linear model will not be useful because of the restrictions it imposes on the covariance structure.

\subsection{LMM definition}
\label{subsec : lmm_definition}
Following the notations from \citet{lesaffre_bayesian_2012}, the LMM for the observations of the $i^{th}$ subject among the $n$ subjects is given by

\begin{equation}
\label{eq : lmm_definition}
\boldsymbol{y}_i = \boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_{i} + \boldsymbol{\varepsilon}_{i}
\end{equation}

where $1 \le i \le n$,\\
$\boldsymbol{y}_i = {(y_{i1}, y_{i2}, \ldots, y_{im_i})}^T$ is a vector of observations for the $i^{th}$ subject taken at $m_i$ time points,\\
$\boldsymbol{X}_i = {(\boldsymbol{x}_{i1}^T, \boldsymbol{x}_{i2}^T, \ldots, \boldsymbol{x}_{im_i}^T)}^T$ is the $m_i \times (d+1)$ design matrix for the $i^{th}$ subject,\\
$\boldsymbol{\beta} = {(\beta_0, \beta_1, \ldots, \beta_d)}^T$ is a $(d+1) \times 1$ vector of fixed effects with $\beta_0$ being the intercept,\\
$\boldsymbol{Z}_i = {(\boldsymbol{z}_{i1}^T, \boldsymbol{z}_{i2}^T, \ldots, \boldsymbol{z}_{im_i}^T)}^T$ is the $m_i \times q$ design matrix of covariates multiplying the random effects,\\
$\boldsymbol{b}_i = {(b_{0i}, b_{1i}, \ldots, b_{(q-1)i})}^T$ is a $q \times 1$ vector of random effects with $b_{0i}$ being the random intercept. The random effects $\boldsymbol{b}_i \sim N_q(\boldsymbol{0}, G)$ with $G$ being the $q \times q$ covariance matrix,\\ 
$\boldsymbol{\varepsilon}_{i} = {(\varepsilon_{i1}, \varepsilon_{i2}, \ldots, \varepsilon_{im_i})}^T$ is a $m_i \times 1$ vector of measurement errors. The errors $\boldsymbol{\varepsilon}_{i} \sim N_{m_i}(\boldsymbol{0}, R_i)$ with $R_i$ being the $(m_i \times m_i)$ covariance matrix of errors,\\

The errors $\boldsymbol{\varepsilon}_{i}$ and the random effects $\boldsymbol{b}_i$ are assumed to be independent. $R_i$ is usually a diagonal matrix of the form $\sigma^2I_{m_i}$. While one might only model the correlation between the observations of a subject using random effects, it is also possible to model the serial correlation component.

\section{Motivation for Bayesian linear mixed model}
\label{sec : blmm}
One of issues with the frequentist LMM is that while the parameters in matrices $G$ and $R_i$ are estimated using ML/REML only a point estimate is further used in estimation of fixed effects(see \cite[chap. 5]{verbeke_linear_2009}). Hence the uncertainty in estimation of random effects is ignored. Although frequentist inference approaches try to mitigate this issue by modifying the distributional assumptions of the test statistic \citep[pg. 56]{verbeke_linear_2009}, a bayesian approach considers the variability in parameter estimates in the first place. A similar problem occurs in the estimation of $\boldsymbol{b}_i$. The frequentist strategy is to use Empirical bayes estimates where the the posterior distribution of random effects uses point estimates of parameters in matrices $G$ and $R_i$. Thus the uncertainty in estimation is ignored. On the other hand the bayesian approach averages out over the entire posterior distribution of the hyperparameters to obtain the posterior $p(\boldsymbol{b}_i|\boldsymbol{y})$. In light of these reasons, in this thesis we will model our data using Bayesian linear mixed models.\\

The Bayesian linear mixed model or BLMM can be obtained by assigning a distribution to all the parameters involved in a LMM. This means that for the model presented in section \ref{subsec : lmm_definition} we will have a prior distribution for the following:
\begin{itemize}
\item $\sigma^2 \sim p(\sigma^2)$
\item $\boldsymbol{\beta} \sim p(\boldsymbol{\beta})$
\item $G \sim p(G)$
\end{itemize}

\section{Motivation for mixture of random effects}
As we saw above the random effects are assumed to be multivariate normally distributed. It could be too strong an assumption though in certain cases. A classical example of it are the longitudinal studies where at any time point we would like to categorize subjects in groups. For e.g. group with a high risk of having a certain disease in future vs. group with a low risk. While in retrospective studies it is quite easy as we know exactly which patients were diagnosed with the disease and which were not. However in a study where we would like to categorize patients into different groups well before diagnosis this could be difficult. Here is a toy example for it. Imagine that in longitudinal study we are measuring a response $Y$ which is an indicator of a disease. Assume that from a previous study it is known that patients which are in high risk group for the disease tend to have a higher response $Y$ during all times. Also assume that the trend of $Y$ over time remains the same for both groups otherwise. Figure \ref{fig : random_slope_dummy_data} shows individual profiles of subjects from a simulated dataset. Looking at this plot we can say that a random intercept component will be enough to model individual profiles. Since we will not be knowing which patient belongs to which group, this heterogeneity can be appropriately modeled by considering that the random intercept is a mixture of two normal components.\\

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{mainmatter/chapter_3_blmm/random_slope_dummy_data.png}
	\caption{Individual profiles of 30 subjects from each group.}
	\label{fig : random_slope_dummy_data}
\end{figure}

In a LMM is quite common to use histogram of Empirical Bayes estimates of random effects to detect groups of individuals. However \citet{verbeke_linear_1996} have shown that if the prior is misspecified(for e.g. if in our example we use a univariate normal distribution), then the histogram of estimates of random effects will be shrunk towards the prior distribution. Thus it would be impossible to classify the subjects into different categories based on empirical bayes estimates of random effects as they are incorrect. A solution to this problem is using a mixture of Gaussian components for random effects distribution. Such a linear mixed model is termed as a Heterogeneity model.

\subsection{Bayesian heterogeneity model}
\label{subsec : bhtge}
The formal definition of a Bayesian heterogeneity model can be given by extending the Bayesian linear mixed model definition given in section \ref{sec : blmm}. Since, now the random effects have a Gaussian mixture distribution we will use the following notation to express the distribution mathematically.

$$\boldsymbol{b}_i \sim \sum_{k=1}^{K} \eta_k N_q(\boldsymbol{b}_k^C, G_k)$$\\
where $\boldsymbol{b}_k^C$ and $G_k$ are the mean vector and covariance matrices for the $k^{th}$ component in the mixture distribution respectively. The vector $\boldsymbol{\eta} = (\eta_1, \eta_2, \ldots, \eta_K)$ is the weight distribution for the component densities. The vector $\boldsymbol{S}=(S_1, S_2, ..., S_n)$ represents the allocation vector for all of the subjects. Since we are following the bayesian paradigm, in addition to prior distribution for $\boldsymbol{\beta}$ and $\sigma^2$ we also have prior for $\boldsymbol{\nu} = (\boldsymbol{b}_1^C, \boldsymbol{b}_2^C, \ldots, \boldsymbol{b}_K^C, \boldsymbol{\eta}, G_1, G_2, \ldots, G_K)$.


\section{Estimation of parameters in the Bayesian heterogeneity model}
In this section we will discuss some of the challenges in Bayesian estimation of parameters in the Bayesian heterogeneity model. We will also discuss the approaches we used to deal with them in this thesis.

\subsection{Marginal vs. Hierarchical model}
Suppose that in our heterogeneity model we know the alloation vector $S_i$ for each subject. Then conditional on knowing $S_i=k$ the following LMM equation has a hierarchical interpretation.

$$\boldsymbol{y}_i|\boldsymbol{b}_{i} \sim N(\boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_{i},\boldsymbol{\varepsilon}_{i}), 
\boldsymbol{\varepsilon}_{i} \sim N_{m_i}(\boldsymbol{0}, R_i)$$

One can however integrate out the random effects $\boldsymbol{b}_{i}$ and obtain the corresponding marginal Bayesian heterogeneity model,

$$\boldsymbol{y}_i \sim N(\boldsymbol{X}_{i}\boldsymbol{\beta} + \boldsymbol{Z}_{i}\boldsymbol{b}_k^C, \boldsymbol{\varepsilon}_{i}^*), 
\boldsymbol{\varepsilon}_{i} \sim N_{m_i}(\boldsymbol{0}, \boldsymbol{Z}_{i}G_k\boldsymbol{Z}_{i}^T+ R_i)$$

The marginal model is recommended by \citet{fruhwirth-schnatter_bayesian_2004} for good mixing of chains, and while doing the simulation study(presented in chapter \ref{ch : simulation_study}) we found that claim to be true. However the Marginal model took quite a long time to for each iteration. Secondly it did not give posterior estimates of the random effects $\boldsymbol{b}_i$ which were required for calculation of certain definitions of DIC (discussed in chapter \ref{ch : model_selection}). Besides we found that a model with hierarchical centering had as much autocorrelated estimates as the marginal model and took less time for each iteration.

\subsection{Hierarchical centering}
The random effects $\boldsymbol{b}_i$ in a mixed model could be seen as random deviations from the fixed effects$(\boldsymbol{\beta})$ with a mean $\boldsymbol{0}$. For a longitudinal data set, it means that the overall effect of a covariate like time for a subject should be the sum of both fixed and random effects. In this case matrices $\boldsymbol{X}$ and $\boldsymbol{Z}$ both share columns corresponding to the variable time. To enforce the mean $\boldsymbol{0}$ on the random effects in a mixture distribution the following condition should be satisfied.

\begin{equation}
\label{eq : non_centred_constraint}
E(\boldsymbol{b}_i | \boldsymbol{\phi}) = \sum_{k=1}^{K} \eta_k N_q(\boldsymbol{b}_k^C, G_k) = 0
\end{equation}

where $\boldsymbol{\phi}$ is the vector $(\boldsymbol{b}_1^C, \boldsymbol{b}_2^C, \ldots, \boldsymbol{b}_K^C, \boldsymbol{\eta}, G_1, G_2, \ldots, G_K)$. This further means that $E(\boldsymbol{y}_i | \boldsymbol{\phi}) = \boldsymbol{X}_{i}\boldsymbol{\beta}$. This parametrization, which was also used in the original paper on Heterogeneity model \citep{verbeke_linear_1996} is called noncentralized parametrization. The centralized parametrization assumes that the random effects are not deviations from the fixed effects and are centred around a non zero mean.
The choice of parametrization has an effect on the rate of convergence while estimating parameters using MCMC. While doing the simulation study we figured out that imposing the constraint in equation \ref{eq : non_centred_constraint} drastically slows the convergence as well increases the autocorrelation in parameter estimates. Thus, in this thesis we have only used hierarchically centred parametrization.

\subsection{Choice of priors}
\label{subsec : choice_priors}
Since we are following a Bayesian paradigm parameters in the Bayesian heterogeneity model are random variables and thus need to have a prior distribution. There are certain difficulties in specifying the prior though, especially that it can be difficult to implement theoretically preferred prior distributions. As an example we will begin with the choice of prior for the mean $(b_k^C)$ and covariance matrix $(G_k)$ of components densities in the mixture distribution of random effects. We know that both $(b_k^C)$ and $(G_k)$ are unknown, and hence to obtain the joint posterior in closed form one has to specify the conditionally conjugate prior $b_k^C | G_k \sim N(\boldsymbol{\mu}_0, \frac {G_K} {N_0})$ and $G_k^{-1} \sim \mathcal{W} (n_0, \Psi)$. The marginal prior of $b_k^C$ is a multivariate T distribution. However the problem with this approach is that it is firstly difficult to implement in JAGS and even if one does, the extra computationally intensive procedure does not provide much advantage in practice. It is thus a widespread practice to use independent priors for mean and covariance matrix \citep[chap. 17]{gelman_data_2006}. For e.g. a common non informative prior for $b_k^C$ (say, having only random intercept and slope) is $N(\boldsymbol{0}, \begin{bmatrix}10^5 & 0 \\ 0 & 10^5\end{bmatrix})$. This prior is equivalent to specifying indepedent diffuse univariate normal priors for random intercept mean and random slope mean.

\subsubsection{Choice of prior for covariance matrix}
The choice of prior for the covariance matrix is an interesting one. \citet[pg. 260]{lesaffre_bayesian_2012} suggest using an inverse wishart prior with small diagonal elements for the scale hyperparameter and degrees of freedom hyperparameter equal to the dimension of parameter at hand. For e.g $IW(\begin{bmatrix}0.01 & 0 \\ 0 & 0.01\end{bmatrix}, 2)$ could be one such prior. For precision matrix one can use the Wishart prior $W(\begin{bmatrix}10 & 0 \\ 0 & 10\end{bmatrix}, 2)$. i.e. the scale is inverse of the scale hyperparameter for inverse wishart distribution. As we found later in our simulations, a big value for diagnoal elements of scale matrix of Wishart distribution influence the posterior more than the likelihood does.\\

One can also choose indepdent gamma priors for random intercept and random slope and uniform prior $U(-1,1)$ for correlation between the two. The upside of this approach is that it gives almost the same estimates as one can get from frequentist analysis, but the downside is that MCMC iterations are slow as the posterior is not available as a known density. Another benefit of this approach, as we later found out during simulations is that when more components than needed are fitted to the data, then the extra components tend to have very high variance estimates for random intercept and random slope. This property can be used to make decisive posterior predictive checks.

\subsubsection{Choice of priors for $\boldsymbol{\beta}$ and $\sigma^2$}
We assume that the parameters $\boldsymbol{\beta}$ and $\sigma^2$ are indepdent from $\boldsymbol{b}_1^C, \boldsymbol{b}_2^C, ..., \boldsymbol{b}_K^C, G_1, G_2, ..., G_K$. The problem of choosing a conjugate prior such that the joint posterior of $(\boldsymbol{\beta}, \sigma^2)$ is available as a known density, is similar to the problem we discussed in previous two subsections. It is quite common in practice to use indepdent univariate normal priors such as $N(0, 10000)$ for each of the $\beta_p$ and a $\text{Gamma}(0.0001, 0.0001)$ prior for $\tau = \frac 1 {\sigma^2}$ \citep[chap. 17]{gelman_data_2006}.

\subsubsection{Choice of prior for $\boldsymbol{\eta}$}
The conjugate prior for the vector of mixture weights $\boldsymbol{\eta}$ is the Dirichlet prior. \citet{fruhwirth-schnatter_finite_2013} suggest choosing values of hyperparameters $(a_0, a_1,..., a_K)$ to be greather than 1 for cases where one of the components is nearly empty. If one chooses the hyperparameters to be equal to 1 then label switching is observed whenever one of the components is nearly empty. Having larger values although mitigated label switching but gave estimates far from the actual one. 

\subsection{Label Switching}
We use a mixture distribution for random effects in the Bayesian heterogeneity model. However we do not know the allocation vector $\boldsymbol{S}$ in advance. In this case the mixture likelihood is given by equation \ref{eq : obs_data_likelihood}. This likelihood function is symmetrical and has $K!$ modes \citep[pg. 44]{fruhwirth-schnatter_finite_2013}. This creates a problem called label switching while doing the MCMC procedure.\\

To idea of label switching could be explained with this simple example. Suppose we have a mixture distribution $0.5N(5,1) + 0.5N(7,1)$of two components $C_1$ and $C_2$ and we sampled a few observations from it. Using the MCMC procedure we will estimate the parameters of the two components. The MCMC procedure for missing data models like mixture models uses a technique called data augmentation. The idea of data augmentation is similar to the frequentist EM algorithm. ie. we begin with some random allocation vector $\boldsymbol{S}^0$ and estimate parameters using complete data likelihood. An example of a complete data likelihood for Bayesian heterogeneity model is expression \ref{eq : complete_data_likelihood}. For the MCMC sampler labels $\mu_1$ and $\mu_2$ exist for the two means, however any of the two can correspond to $\mu_{C1}$ or $\mu_{C2}$. i.e. Labels are not associated with actual components. Assume that the allocation vector we began with assigns all observations from component $C_1$ under label 1 and all observations from component $C_2$ under label 2. Under such a scheme $(\mu_1,\mu_2) = (5,7)$ is likely. However if we take a conjugate of this allocation vector $(\mu_1,\mu_2) = (7,5)$ will also be accepted. This because we have a mixture likelihood function which is bimodal.\\

Now let us imagine a scenario where because of our initial allocation vector, parameter estimates are $(\mu_1, \mu_2) = (5.5,6.5)$. So far it seems $\mu_1$ represents $\mu_{C1}$ and $\mu_2$ represents $\mu_{C2}$. Now we estimate allocation vector conditional on these estimates in MCMC. Supposing that an observation with value 6.5 originally from component $C_2$ gets allocated to component $C_1$ and similarly an observation with value 5.5 from component $C_1$ gets allocated to $C_2$. Unless we impose some constraint like $\mu_1 < \mu_2$, under the current situations even $(\mu_1,\mu_2) = (6.5, 5.5)$ could be sampled by MCMC. If the sampler keeps on arbitrarily switching between the two equivalent posterior regions then because of this label switching, one may obtain a bimodal posterior for both $\mu_1$ and $\mu_2$. Thus both posteriors will be partially explored and any inference based on this posterior will be useless.

\subsubsection{Dealing with label switching}
One of the techniques we used for dealing with label switching was imposing an identifiability constraint such as $\mu_1 < \mu_2$. The difficulty with this approach is that it is easier to do in univariate mixtures but not with multivariate mixtures. For e.g. a multivariate mixture that we have is mixture distribution for joint distribution of random intercept and random slope. In this thesis we employ identifiability constraint only on either the random intercept or the random slope depending upon the variance of each random effect. It is interesting to note that if more components than needed are chosen than label swtiching is unavoidable, and should also be seen as an indicator for overfitting \citep[pg. 104]{fruhwirth-schnatter_finite_2013}.\\

Postprocessing of MCMC chains by relabeling with output has also been suggested in the literature \citep{richardson_bayesian_1997,stephens_dealing_2000}. We also employed this method in the calculation of Bayes Factor, as that also involves running further MCMC chains (expression \ref{eq : rao_blackwellization_wishart}). Without careful relabeling of output one can obtain a Bayes factor $\to 0$ and thus reject the model outright.