% !TEX root =  ../../thesis.tex
\chapter{Model selection criteria}
\label{ch : model_selection}

In most cases we do not know the right number of mixture components in advance unless we have some expert knowledge available or we know them from a previous/similar study. As part of this thesis we will compare many of the existing methods for finding the right number of mixture components.

\section{Deviance information criteria}
\label{sec : dic}

The Deviance information criteria or DIC was first proposed by \citet{spiegelhalter_bayesian_2002} for Bayesian model selection. The idea is similar to frequentist AIC/BIC criteria in the sense that DIC also penalizes more elaborate models using a penalty component. The definition for DIC is given by 

$$\text{DIC} = -2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})} + 2\text{p}_\text{D}$$
where $\boldsymbol{\bar{\theta}} = \text{E}(\boldsymbol{\theta}|\boldsymbol{y})$,\\
$\text{p}_\text{D} = -2\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\theta}|\boldsymbol{y}])}) + \log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})})$ is the penalty for model complexity, and can also be written as 

$$\text{p}_\text{D}=\overline{D(\boldsymbol{\theta})} - D(\boldsymbol{\bar{\theta}})$$

where $D(\boldsymbol{\theta}) = -2\log{p(\boldsymbol{y}|[\boldsymbol{\theta}|\boldsymbol{y}])} + 2\log{f(\boldsymbol{y})}$ is called the Bayesian deviance. The term $f(\boldsymbol{y})$ however cancels out in the expression for $\text{p}_\text{D}$ and hence is not discussed.

\subsection{DIC for missing data models}
\label{subsec : DIC_missing_data_models}
Mixture models and mixed models both are both a member of the clas of models called missing data models. The reason is that the allocation vector $\boldsymbol{S}$ in a mixture model and matrix of random effects $\boldsymbol{b}=(\boldsymbol{b}_1, \boldsymbol{b}_1, ..., \boldsymbol{b}_n)$ in a LMM, both are not observed directly. Thus one could have various incompatible definitions of DIC based on observed data likelihood, complete data likelihood and conditional data likelihood, as shown by DeIorio and Robert in a discussion on the paper of \citet{spiegelhalter_bayesian_2002}. Further, \citet{celeux_deviance_2006} proposed multiple definitions of DIC under each of the aforementioned likelihood classes and showed that each has a different value and thus different impact on model selection. In this thesis we will take some of those definitions and apply them in context of the Bayesian heterogeneity model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Observed Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Observed DIC}
The first category of DIC's is associated with observed data likelihood $f(\boldsymbol{y}|\boldsymbol{\theta})$ or in our case $f(\boldsymbol{y}|\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu})$, where $\boldsymbol{\nu}$ is as defined in section \ref{subsec : bhtge}. The observed likelihood can be obtained by marginalizing over the allocation vector of subjects $\boldsymbol{S}$ and random effects $\boldsymbol{b}$. This give us the following formula for observed data likelihood.

\begin{equation}
\label{eq : obs_data_likelihood}
f(\boldsymbol{y}|\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}) = \prod_{i=1}^n \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_k^C, \boldsymbol{Z}_{i} G_k \boldsymbol{Z}_{i}^T+ R_i) \eta_k
\end{equation}
 
Based on equation \ref{eq : obs_data_likelihood} we will now extend the definition of $\text{DIC}_1$, $\text{DIC}_2$ and $\text{DIC}_3$ proposed by \citet{celeux_deviance_2006} to give

\begin{equation}
\label{eq : DIC1}
\text{DIC}_1 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}=\text{E}(\boldsymbol{\beta}|\boldsymbol{y})$, 
$\bar{\sigma^2}=\text{E}(\sigma^2|\boldsymbol{y})$ and 
$\boldsymbol{\bar{\nu}}=\text{E}(\boldsymbol{\nu}|\boldsymbol{y})$,\\

$\text{DIC}_2$'s definition is similar to $\text{DIC}_1$ but intead of posterior mean, posterior mode is used in calculation of $D(\boldsymbol{\bar{\theta}})$. It is given by

\begin{equation}
\label{eq : DIC2}
\text{DIC}_2 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\beta}}, \hat{\sigma^2}, \boldsymbol{\hat{\nu}})})
\end{equation}

where
$\boldsymbol{\hat{\beta}} = \argmax_{\boldsymbol{\beta}}p(\boldsymbol{\beta}|\boldsymbol{y})$,
$\hat{\sigma^2} = \argmax_{\sigma^2}p(\sigma^2|\boldsymbol{y})$ and
$\boldsymbol{\hat{\nu}} = \argmax_{\boldsymbol{\nu}}p(\boldsymbol{\nu}|\boldsymbol{y})$,\\

\citet{celeux_deviance_2006} suggest that for models where non identifiability of parameters is endemic, as is the case for mixtures usually, one should use an estimator $\hat{f}(\boldsymbol{y})$ for the approximation of the density $p(\boldsymbol{y} | \boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu})$. They further propose the following estimator for $\hat{f}(\boldsymbol{y})$ which uses posterior samples $\boldsymbol{\theta}^{(l)}$ from the $l^{\text{th}}$ MCMC iteration of a chain of length $m$.

$$\hat{f}(\boldsymbol{y}) = \prod_{i=1}^n \hat{f}(\boldsymbol{y}_i) = \prod_{i=1}^n \frac 1 m \sum_{l=1}^m \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta}^{(l)} + \boldsymbol{Z}_i {\boldsymbol{b}_k^C}^{(l)}, \boldsymbol{Z}_{i} G_k^{(l)} \boldsymbol{Z}_{i}^T+ R_i^{(l)}) \eta_k^{(l)}$$

This gives us the following definition of DIC.

\begin{equation}
\label{eq : DIC3}
\text{DIC}_3 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{\hat{f}(\boldsymbol{y})}
\end{equation}
\todo[inline]{The other interesting aspect of this exercise was that for $\text{DIC}_3$ hasd a positive pd}
In each of the equations \ref{eq : DIC1}, \ref{eq : DIC2}, \ref{eq : DIC3}, the calculation of $\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) $ can be done by approximating it using the results from the MCMC iterations in the following way.

\begin{equation}
\label{eq : mean_posterior_deviance_approx}
\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])})  = \frac 1 m \sum_{l=1}^m\log{p(\boldsymbol{y}|\boldsymbol{\beta}^{(l)}, {\sigma^2}^{(l)}, \boldsymbol{\nu}^{(l)})})
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Complete Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Complete DIC}
The second class of the DIC is based on the complete data likelihood. Complete data for the $i^\text{th}$ subject in a Bayesian heterogeneity model will be $(\boldsymbol{y}_i, S_i, \boldsymbol{b}_i)$. The following equation shows the complete data likelihood of the data at hand.

\begin{equation}
\label{eq : complete_data_likelihood}
f(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S} | \boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}) = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i) f_N(\boldsymbol{b}_i; \boldsymbol{b}_{S_i}^C, G_{S_i}) \eta_{S_i}
\end{equation}

The formulation of complete data DIC is straightforward as we assume $(\boldsymbol{b}, \boldsymbol{S})$ to be observed. It can be written down as,

\begin{equation}
\label{eq : complete_data_dic}
\text{DIC} = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}=\text{E}(\boldsymbol{\beta}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$, 
$\bar{\sigma^2}=\text{E}(\sigma^2|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$ and 
$\boldsymbol{\bar{\nu}}=\text{E}(\boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$,\\

Unfortunately $(\boldsymbol{b}, \boldsymbol{S})$ are latent and thus \citet{celeux_deviance_2006} propose integrating the expression in \ref{eq : complete_data_dic} with respect to $(\boldsymbol{b}, \boldsymbol{S})$ to obtain the following definition of DIC.

\begin{equation}
\label{eq : DIC4}
\text{DIC}_4 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu},\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\text{E}_{\boldsymbol{b},\boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}$, $\bar{\sigma^2}$ and $\boldsymbol{\bar{\nu}}$ remain same as for \ref{eq : complete_data_dic}. The first part of the formula for $\text{DIC}_4$ is not available in closed form for Bayesian heterogeneity model, however it can still be approximated using the output of Gibbs sampler in the same way as in \ref{eq : mean_posterior_deviance_approx}. However although one has to also simulate $(\boldsymbol{b}, \boldsymbol{S})$ in the MCMC iterations. The reason it works is that during each iteration of the Gibbs sampler, it simulates parameter values from the conditional distribution of the parameters. i.e. conditional on every other parameter being simulated in the chain, including the unobserved data. We further verified this approach by comparing the results of $\text{DIC}_4$ approximation for mixture distribution given by \citet{celeux_deviance_2006} with our approach and found the results to be differing only by a few decimal places.\\

The second part of $\text{DIC}_4$, i.e. $\text{E}_{\boldsymbol{b},\boldsymbol{S}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})$, is also not straightforward to compute. While the expectation over $\boldsymbol{b}, \boldsymbol{S}$ can be approximated in the same way as in \ref{eq : mean_posterior_deviance_approx} but for calculating $\boldsymbol{\bar{\beta}}$, $\bar{\sigma^2}$ and $\boldsymbol{\bar{\nu}}$, \citet{celeux_deviance_2006} suggest using the posterior estimates $(\boldsymbol{b}, \boldsymbol{S} | \boldsymbol{y})$ of the unobserved data. We will now give the formulae for the expected values of parameters of interest during the $l^\text{th}$ iteration $\boldsymbol{\bar{\beta}}^{(l)}$, $\bar{\sigma^2}^{(l)}$ and $\boldsymbol{\bar{\nu}}^{(l)}$.

$$\boldsymbol{\bar{\beta}}^{(l)} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{Zb^{(l)}})$$
$$\bar{\sigma^2}^{(l)} = \frac {(\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})^T (\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})} {(\sum_{i=1}^n m_i - p - 1) -2}$$
$$\bar{\boldsymbol{b}_k^C}^{(l)} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) \boldsymbol{b_i^{(l)}}} {n_k^{(l)}}$$
$$\bar{G_k^{(l)}} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) (\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})(\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})^T} 
{(n_k^{(l)} - 1) - \text{rank}(\bar{\boldsymbol{b}_k^C}^{(l)}) - 1}$$
$$\bar{\eta}_k^{(l)} = \frac {a_k + n_k^{(l)}} {\sum_{u=1}^K a_u + n}$$

The next definition of DIC under the class of complete data DIC's is motivated by the fact that the at times $\text{E}(\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y})$ takes values outside the support of the joint distribution of $\boldsymbol{b}, \boldsymbol{S}$ \citet{celeux_deviance_2006}. Thus using MAP(maximum a posteriori) as the estimate instead, the following definition of DIC is proposed.

\begin{equation}
\label{eq : DIC5}
\text{DIC}_5 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu},\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\log{p(\boldsymbol{y}, \boldsymbol{\hat{b}}, \boldsymbol{\hat{S}}|\boldsymbol{\hat{\beta}}, \hat{\sigma^2}, \boldsymbol{\hat{\nu}})}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conditional Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Conditional DIC}
The third class of the DIC is based on the assumption that missing data i.e. allocation vector $\boldsymbol{S}$ and random effects $\boldsymbol{b}_i$ can be seen as additional parameter rather than as missing data. We will represent the new posterior parameter space as $\boldsymbol{\theta}_\text{cond} = $($\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}, \boldsymbol{S}, \boldsymbol{b}_1, \boldsymbol{b}_2, ..., \boldsymbol{b}_n)$. This leads to the conditional data likelihood,

\begin{equation}
\label{eq : conditional_data_likelihood}
f(\boldsymbol{y}|\boldsymbol{\theta}_\text{cond}) = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i)
\end{equation}

Based on this conditional likelihood, \citet{celeux_deviance_2006} proposed the following DIC definition.

\begin{equation}
\label{eq : DIC6}
\text{DIC}_6 = -4\text{E}_{\boldsymbol{\theta}_\text{cond}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\theta}_{\text{cond}}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\theta}}_\text{cond})})
\end{equation}

where
$\boldsymbol{\hat{\theta}}_\text{cond} = \argmax_{\boldsymbol{\theta}_\text{cond}}p(\boldsymbol{\theta}_\text{cond}|\boldsymbol{y})$, and $\text{E}_{\boldsymbol{\theta}_\text{cond}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\theta}_{\text{cond}})}|\boldsymbol{y}])$ can be approximated as done in equation \ref{eq : mean_posterior_deviance_approx}.

\section{Marginal Likelihood}
\label{sec : marginal_likelihood}

The marginal likelihood of data represents the probablilty of data given the model. This can be calculated by margilizing over the model parameters $\boldsymbol{\theta}$. 

\begin{equation}
\label{eq : marginal_likelihood}
p(\boldsymbol{y}|M) = \int p(\boldsymbol{y}|\boldsymbol{\theta}, M) p(\boldsymbol{\theta}|M) \diff \boldsymbol{\theta}
\end{equation}

Given two proposed models for the data, $M_1$ and $M_2$, one can further use the quantity in equation \ref{eq : marginal_likelihood} to calculate model evidence. The idea is to calculate the odds of model $M_1$ against the model $M_2$ given the data. i.e. Posterior odds. This ofcourse means that it is a comparative measure as $\sim M_1 = M_2$. One can write the posterior odds as

$$\frac {p(M_1|\boldsymbol{y})}{p(M_2|\boldsymbol{y})} = \frac {p(\boldsymbol{y}|M_1) p(M_1)} {p(\boldsymbol{y}|M_2) p(M_2)}$$

where $\frac {p(M_1)}{p(M_2)}$ is called prior odds, and $\frac {p(\boldsymbol{y}|M_1)} {p(\boldsymbol{y}|M_2)}$ is called the Bayes Factor.\\

Since we have the same belief in each of these models, prior odds is equal to 1 apriori. To calculate the Bayes Factor we will use the method proposed by \citet{chib_marginal_1995}. Chib's idea is that one can rewrite the Bayes rule in equation \ref{eq : bayes_rule} to get the marginal likelihood formula as

\begin{equation}
\label{eq : chib_trick}
m(\boldsymbol{y}) = p(\boldsymbol{y}|M) = \dfrac {L(\boldsymbol{\theta}|\boldsymbol{y}, M) p(\boldsymbol{\theta}|M)} {p(\boldsymbol{\theta}|\boldsymbol{y}, M)}
\end{equation}

Equation \ref{eq : chib_trick} is valid for all $\boldsymbol{\theta}$, though Chib recommends using posterior mode $\argmax_\theta p(\boldsymbol{\theta}|\boldsymbol{y})$ of parameters or the maximum likelihood estimate $\argmax_\theta L(\boldsymbol{\theta}|\boldsymbol{y})$. We decided to choose the latter of the two. Further, in context of the Bayesian heterogeneity model, we will denote the selected parameter values as ${\boldsymbol{\beta}}^*$, ${\sigma^2}^*$ and $\boldsymbol{\nu}^*$. Thus Chib's approximation for marginal likelihood on log scale is given by,

\begin{equation}
\label{eq : chib_approx}
\log{\hat{m}(\boldsymbol{y})} = \log{L({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})} + \log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*)} - \log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})}
\end{equation}

Note that we have dropped the model indicator $M$ from equation \ref{eq : chib_approx} for readability. We will now show calculations for determing the marginal likelihood value using Chib's approxmiation.\\ 

Firstly $\log{L({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})} = f(\boldsymbol{y}|{\boldsymbol{\beta}}^*, \sigma^2, {\boldsymbol{\nu}}^*)$ can be easily determined using the formula given in equation \ref{eq : obs_data_likelihood}. As for the calculation of $\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*)}$, it is also straightforward as we take independent priors for these parameters and they are well known in advance. The details of the priors we chose are given in section \ref{subsec : choice_priors}. Assuming that the parameters of component densities of the mixture distribution of random effects are indepdent, one can use the following to calculate $\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})}$.

\begin{multline}
\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})} = 
\sum_{k=1}^K{\log{p(G_k^*|\boldsymbol{y})}} + 
\sum_{k=1}^K{\log{p({\boldsymbol{b}_k^C}^*|G_k^*, \boldsymbol{y})}} + 
\log{p({\sigma^2}^*|G_k^*, {\boldsymbol{b}_k^C}^*, \boldsymbol{y})}\\
+ \log{p({\boldsymbol{\beta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*, \boldsymbol{y})} + 
\log{p({\boldsymbol{\eta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*,{\boldsymbol{\beta}}^*, \boldsymbol{y})}
\end{multline}

An interesting problem one faces in such an expansion is that the posteriors may not be available as well known density. For e.g. we began with choosing indepdent gamma priors for precision parameters of random effects and uniform prior for correlation. However the posterior density was not well known. One could try to fit it with a wrapper density however as we will show ahead this is practically improbable. An obvious alternative is to choose conjugate priors in such situation. However as we mentioned in section \ref{subsec : choice_priors} the joint conjugate prior in the case of unknown mean and precision matrix is a Normal-Wishart-Prior and the joint posterior is a Normal-Wishart-Posterior. Although one does not use them in practice while using BUGS family of software, the problem of posterior being from a unknown family remains the same. \citet{chib_marginal_1995} suggested using the Rao-Blackwellization method to solve this problem. For e.g. the Rao-Blackwellized estimate of $p(G_k^*|\boldsymbol{y})$ is given by

\begin{equation}
\label{eq : rao_blackwellization_wishart}
\begin{split}
\prod_{k=1}^K p(G_k^*|\boldsymbol{y}) & = \int \prod_{k=1}^{K} p(G_k^*| \boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}, \boldsymbol{b}_k^C) p(\boldsymbol{b}_1^C, \boldsymbol{b}_2^C,..., \boldsymbol{b}_K^C,\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}) 
\diff{\boldsymbol{b}_1^C} \diff{\boldsymbol{b}_2^C} ... \diff{\boldsymbol{b}_K^C} \diff{\boldsymbol{b}} \diff{\boldsymbol{S}}\\
& \approx \frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} p(G_k^*| \boldsymbol{y}, \boldsymbol{b}^{(l)}, \boldsymbol{S}^{(l)}, {\boldsymbol{b}_k^C}^{(l)
})\\
& \approx \frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} f_{\mathcal{W}^{-1}}(G_k^*; n_k^{(l)}+n_0, \Psi + \sum_{i=1}^{n_k^{(l)}}(\boldsymbol{b_i}^{(l)} - {\boldsymbol{b}_k^C}^{(l)})(\boldsymbol{b_i}^{(l)} - {\boldsymbol{b}_k^C}^{(l)})^T)
\end{split}
\end{equation}

where, $n_k^{(l)}$ are number of subjects classified under component $k$ in iteration $l$ and $(n_0, \Psi)$ are the parameters for the inverse wishart distribution specified as prior for the variance covariance matrix of the component densities. The approximation in \ref{eq : rao_blackwellization_wishart} is done by approximating the integral with the samples obtained from the MCMC iterations. As we can see the benefit of this approach is that $p(G_k^*| \boldsymbol{y}, \boldsymbol{b}^{(l)}, \boldsymbol{S}^{(l)}, {\boldsymbol{b}_k^C}^{(l)})$ is the well known inverse wishart density. However in cases when this posterior is not well known, then given that the large number of MCMC iterations one does, it is not possible to manually check and fit wrapper densities to posterior densities. The use kernel density estimation procedures can also be dismissed as they require significant computational power. It is because of these reasons we avoided calculation of Bayes factor in the case where we took indepdent gamma priors for precision of random effects and uniform prior for correlation.\\

Proceeding further with the Rao-Blackwellization procedure one can obtain the following approximations for the other parameters.

\begin{equation}
\prod_{k=1}^K p({\boldsymbol{b}_k^C}^*|G_k^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} f_N({\boldsymbol{b}_k^C}^*; (G_0^{-1} + n_k^{(l)} {G_k^*}^{-1})^{-1} (G_0^{-1}\boldsymbol{\mu}_0 + n_k^{(l)} {G_k^*}^{-1} \bar{\boldsymbol{b}_{ik}^{(l)}}) , (G_0^{-1} + n_k {G_k^*}^{-1})^{-1})
\end{equation}

\begin{equation}
p({\sigma^2}^*|G_k^*, {\boldsymbol{b}_k^C}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_{Inv-Gamma}({\sigma^2}^*; \alpha_0 + \frac {\sum_{i=1}^n m_i} 2, 
\beta_0 + \frac {\sum_{i=1}^n \sum_{j=1}^{m_i} (y_{ij} - \boldsymbol{x_{ij}}\boldsymbol{\beta}^{(l)} - \boldsymbol{z_{ij}}\boldsymbol{b_i^{(l)}})} 2)
\end{equation}

\begin{equation}
p({\boldsymbol{\beta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_N({\boldsymbol{\beta}}^*; (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{y} - {\boldsymbol{Zb}}^{(l)}), {\sigma^2}^*(\boldsymbol{X}^T\boldsymbol{X})^{-1})
\end{equation}

\begin{equation}
p({\boldsymbol{\eta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*,{\boldsymbol{\beta}}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_{Dir}({\boldsymbol{\eta}}^*; {a_0}_1 + n_1^{(l)}, {a_0}_2 + n_2^{(l)}, ..., {a_0}_K + n_K^{(l)})
\end{equation}

where, 
$(\boldsymbol{\mu}_0, G_0)$ are the parameters for the multivariate normal prior for the mean $\boldsymbol{b}_k^C$ of the $k^\text{th}$ component density,\\
$\bar{\boldsymbol{b}_{ik}^{(l)}} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) \boldsymbol{b_i^{(l)}}} {n_k^{(l)}}$ is the mean of the estimated random effects corresponding to the $n_k$ subjects classified under the $k^\text{th}$ component in the $l^\text{th}$ MCMC iteration,\\
$(\alpha_0, \beta_0)$ are the parameters of the inverse gamma density specified as the prior for the within subject variance $\sigma^2$,\\
${a_0}_1, {a_0}_2,..., {a_0}_K$ are the parameters of the Dirichlet density specified as the prior for component weight vector $\boldsymbol{\eta}$.\\

Using these values an estimate of $\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})}$ is available which can be further substituted in equation \ref{eq : chib_approx} to obtain $\log{\hat{m}(\boldsymbol{y})}$. In ideal cases, i.e. where marginal likelihood is known to work well as a model selection criteria, models with higher value of $\log{\hat{m}(\boldsymbol{y})}$ should be chosen.


\section{Posterior predictive checks}
\label{sec : ppc}

The idea of the posterior predictive checks is to evaluate the model fit using simulations from the posterior predictive distribution(PPD) $p(\boldsymbol{\tilde{y}}|\boldsymbol{y}$. As an informal check one could sample 1000 values from the PPD 20 times and make 20 histograms to show the density. If the histograms do not match with the histogram of the original sample one could say that the model did not fit the data well.\\ 
A formal way to do this is using Posterior predictive p-values(PPP) or Bayesian p-values. In the frequentist paradigm after fitting a model based on parameter $\hat{\theta}$ one could test the model using test statistic. Let us represent that test statistic value for original sample to be $T(\boldsymbol{y})$. Now based on the sampling distribution of $T(\boldsymbol{\tilde{y}})$ we could check the probability $P(T(\boldsymbol{\tilde{y}}) > T(\boldsymbol{y})$. In the bayesian paradigm the parameter $\theta$ has a posterior distribution and so we find the same probability like before albeit averaged over the entire posterior $p(\theta|\boldsymbol{y})$. A small PPP value indicates bad fit of model to the data. For a complete interpretation of this p-value we refer the readers to \citet{gelman_understanding_2012}.
