% !TEX root =  ../../thesis.tex
\chapter{Model selection criteria}
\label{ch : model_selection}

In most cases we do not know the right number of components in a mixture distribution in advance. As part of this thesis we will compare 3 of the existing Bayesian methods for finding the right number of mixture components.

\section{Deviance information criteria}
\label{sec : dic}

The Deviance information criteria or DIC was first proposed by \citet{spiegelhalter_bayesian_2002} for Bayesian model selection. The motivation for DIC is similar to frequentist AIC/BIC criteria in the sense that DIC also penalizes more elaborate models using a penalty component. The definition for DIC is given by 

$$\text{DIC} = -2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})} + 2\text{p}_\text{D}$$
where $\boldsymbol{\bar{\theta}} = \text{E}(\boldsymbol{\theta}|\boldsymbol{y})$,\\
$\text{p}_\text{D} = -2\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\theta}|\boldsymbol{y}])}) + \log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})})$ is the penalty for model complexity, and can also be written as 

$$\text{p}_\text{D}=\overline{D(\boldsymbol{\theta})} - D(\boldsymbol{\bar{\theta}})$$

where $D(\boldsymbol{\theta}) = -2\log{p(\boldsymbol{y}|[\boldsymbol{\theta}|\boldsymbol{y}])} + 2\log{f(\boldsymbol{y})}$ is called the Bayesian deviance. The term $f(\boldsymbol{y})$ however cancels out in the expression for $\text{p}_\text{D}$ and hence is not discussed here.

\subsection{DIC for missing data models}
\label{subsec : DIC_missing_data_models}
Mixture models and mixed models both are both a member of the class of models called missing data models. The reason is that the allocation vector $\boldsymbol{S}$ in a mixture model and matrix of random effects $\boldsymbol{b}=(\boldsymbol{b}_1, \boldsymbol{b}_1, ..., \boldsymbol{b}_n)$ in a LMM, both are not observed directly. Thus one could have various incompatible definitions of DIC based on observed data likelihood, complete data likelihood and conditional data likelihood, as shown by DeIorio and Robert in a discussion on the paper of \citet{spiegelhalter_bayesian_2002}. Further, \citet{celeux_deviance_2006} proposed multiple definitions of DIC under each of the aforementioned likelihood classes and showed that each has a different value and a different impact on model selection. In this thesis we will take some of those definitions and apply them on the Bayesian heterogeneity model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Observed Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Observed DIC}
The first category of DIC is associated with observed data likelihood $f(\boldsymbol{y}|\boldsymbol{\theta})$ or in our case $f(\boldsymbol{y}|\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu})$, where $\boldsymbol{\nu}$ is as defined in section \ref{subsec : bhtge}. The observed likelihood can be obtained by marginalizing over the allocation vector of subjects $\boldsymbol{S}$ and random effects $\boldsymbol{b}$. This gives us the following formula for observed data likelihood.

\begin{equation}
\label{eq : obs_data_likelihood}
f(\boldsymbol{y}|\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}) = \prod_{i=1}^n \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_k^C, \boldsymbol{Z}_{i} G_k \boldsymbol{Z}_{i}^T+ R_i) \eta_k
\end{equation}
 
Based on equation \ref{eq : obs_data_likelihood} we will now extend the definition of the various definitions of DC proposed by \citet{celeux_deviance_2006}. The first one is the classical definition for DIC, as shown below.

\begin{equation}
\label{eq : DIC1}
\text{DIC}_1 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}=\text{E}(\boldsymbol{\beta}|\boldsymbol{y})$, 
$\bar{\sigma^2}=\text{E}(\sigma^2|\boldsymbol{y})$ and 
$\boldsymbol{\bar{\nu}}=\text{E}(\boldsymbol{\nu}|\boldsymbol{y})$,\\

One of the problems with $\text{DIC}_1$ is that in case of label switching, the corresponding posteriors may be mutimodal and one may obtain a negative $\text{p}_\text{D}$. In our simulation study we encountered this problem as well (section \ref{subsec : dic_simulation_results}). Since posterior mode is immune to label switched posteriors, the next definition of DIC is formed by replacing posterior mean with posterior mode in the calculation of $D(\boldsymbol{\bar{\theta}})$. 

\begin{equation}
\label{eq : DIC2}
\text{DIC}_2 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\beta}}, \hat{\sigma^2}, \boldsymbol{\hat{\nu}})})
\end{equation}

where
$\boldsymbol{\hat{\beta}} = \argmax_{\boldsymbol{\beta}}p(\boldsymbol{\beta}|\boldsymbol{y})$,
$\hat{\sigma^2} = \argmax_{\sigma^2}p(\sigma^2|\boldsymbol{y})$ and
$\boldsymbol{\hat{\nu}} = \argmax_{\boldsymbol{\nu}}p(\boldsymbol{\nu}|\boldsymbol{y})$,\\

\citet{celeux_deviance_2006} further suggest that for models where non identifiability of parameters is endemic, as is the case for mixtures usually, one should use an estimator $\hat{f}(\boldsymbol{y})$ for the approximation of the density $p(\boldsymbol{y} | \boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu})$. They proposed the following estimator for $\hat{f}(\boldsymbol{y})$ which uses posterior samples $\boldsymbol{\theta}^{(l)}$ from the $l^{\text{th}}$ MCMC iteration of a chain of length $m$.

$$\hat{f}(\boldsymbol{y}) = \prod_{i=1}^n \hat{f}(\boldsymbol{y}_i) = \prod_{i=1}^n \frac 1 m \sum_{l=1}^m \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta}^{(l)} + \boldsymbol{Z}_i {\boldsymbol{b}_k^C}^{(l)}, \boldsymbol{Z}_{i} G_k^{(l)} \boldsymbol{Z}_{i}^T+ R_i^{(l)}) \eta_k^{(l)}$$

This gives us the next definition of DIC shown below. The benefit of using $\text{DIC}_3$ was that it always had a postive $\text{p}_\text{D}$ value. We later found out that $\text{p}_\text{D}$ for $\text{DIC}_3$ never took extreme values; even when models were severely overfitted leading to label switching.

\begin{equation}
\label{eq : DIC3}
\text{DIC}_3 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{\hat{f}(\boldsymbol{y})}
\end{equation}

In each of the equations \ref{eq : DIC1}, \ref{eq : DIC2}, \ref{eq : DIC3}, the calculation of $\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) $ can be done by approximating it using the results from the MCMC iterations in the following way.

\begin{equation}
\label{eq : mean_posterior_deviance_approx}
\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])})  = \frac 1 m \sum_{l=1}^m\log{p(\boldsymbol{y}|\boldsymbol{\beta}^{(l)}, {\sigma^2}^{(l)}, \boldsymbol{\nu}^{(l)})})
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Complete Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Complete DIC}
The second class of the DIC is based on the complete data likelihood. Complete data for the $i^\text{th}$ subject in a Bayesian heterogeneity model will be $(\boldsymbol{y}_i, S_i, \boldsymbol{b}_i)$. The following equation shows the complete data likelihood for the entire dataset.

\begin{equation}
\label{eq : complete_data_likelihood}
f(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S} | \boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}) = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i) f_N(\boldsymbol{b}_i; \boldsymbol{b}_{S_i}^C, G_{S_i}) \eta_{S_i}
\end{equation}

The formulation of complete data DIC is straightforward as we assume $(\boldsymbol{b}, \boldsymbol{S})$ to be observed. It can be written down as,

\begin{equation}
\label{eq : complete_data_dic}
\text{DIC} = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}=\text{E}(\boldsymbol{\beta}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$, 
$\bar{\sigma^2}=\text{E}(\sigma^2|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$ and 
$\boldsymbol{\bar{\nu}}=\text{E}(\boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$,\\

Unfortunately $(\boldsymbol{b}, \boldsymbol{S})$ are latent and thus \citet{celeux_deviance_2006} propose integrating the expression in \ref{eq : complete_data_dic} with respect to $(\boldsymbol{b}, \boldsymbol{S})$ to obtain the following definition of DIC.

\begin{equation}
\label{eq : DIC4}
\text{DIC}_4 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu},\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\text{E}_{\boldsymbol{b},\boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}$, $\bar{\sigma^2}$ and $\boldsymbol{\bar{\nu}}$ remain same as for \ref{eq : complete_data_dic}. The first part of the formula for $\text{DIC}_4$ is not available in closed form for Bayesian heterogeneity model, however it can still be approximated using the output of Gibbs sampler in the same way as in \ref{eq : mean_posterior_deviance_approx}. Though one has to also use the simulated $(\boldsymbol{b}, \boldsymbol{S})$ from the MCMC iterations. The motivation for this approximation is that during each iteration of the Gibbs sampler, it simulates parameter values from the conditional distribution of the parameters. i.e. in our case conditional on the other params and unobserved data both. We further verified the approximation by comparing the results of $\text{DIC}_4$ calculation based on closed form solution for mixture distribution used by \citet{celeux_deviance_2006} and the approximation suggested by them. We found both of the results to be differing only in the decimal places.\\

The second part of $\text{DIC}_4$, i.e. $\text{E}_{\boldsymbol{b},\boldsymbol{S}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})$, is also not straightforward to compute. While the expectation over $\boldsymbol{b}, \boldsymbol{S}$ can be approximated in the same way as in \ref{eq : mean_posterior_deviance_approx} but for calculating $\boldsymbol{\bar{\beta}}$, $\bar{\sigma^2}$ and $\boldsymbol{\bar{\nu}}$, \citet{celeux_deviance_2006} suggest using the posterior estimates $(\boldsymbol{b}, \boldsymbol{S} | \boldsymbol{y})$ of the unobserved data. We will now give the formulae for the expected values of parameters of interest during the $l^\text{th}$ iteration, $\boldsymbol{\bar{\beta}}^{(l)}$, $\bar{\sigma^2}^{(l)}$ and $\boldsymbol{\bar{\nu}}^{(l)}$.

$$\boldsymbol{\bar{\beta}}^{(l)} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{Zb^{(l)}})$$
$$\bar{\sigma^2}^{(l)} = \frac {(\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})^T (\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})} {(\sum_{i=1}^n m_i - p - 1) -2}$$
$$\bar{\boldsymbol{b}_k^C}^{(l)} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) \boldsymbol{b_i^{(l)}}} {n_k^{(l)}}$$
$$\bar{G_k^{(l)}} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) (\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})(\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})^T} 
{(n_k^{(l)} - 1) - \text{rank}(\bar{\boldsymbol{b}_k^C}^{(l)}) - 1}$$
$$\bar{\eta}_k^{(l)} = \frac {a_k + n_k^{(l)}} {\sum_{u=1}^K a_u + n}$$

The next definition of DIC under the class of complete data DIC is motivated by the fact that the at times $\text{E}(\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y})$ takes values outside the support of the joint distribution of $\boldsymbol{b}, \boldsymbol{S}$ \citep{celeux_deviance_2006}. Thus using MAP(maximum a posteriori) as the estimate instead (expression \ref{eq : DIC2}), the following definition of DIC is proposed.

\begin{equation}
\label{eq : DIC5}
\text{DIC}_5 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu},\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\log{p(\boldsymbol{y}, \boldsymbol{\hat{b}}, \boldsymbol{\hat{S}}|\boldsymbol{\hat{\beta}}, \hat{\sigma^2}, \boldsymbol{\hat{\nu}})}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conditional Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Conditional DIC}
The third class of the DIC is based on the assumption that missing data i.e. allocation vector $\boldsymbol{S}$ and random effects $\boldsymbol{b}_i$ can be seen as additional parameter rather than as missing data. We will represent the new posterior parameter space as $\boldsymbol{\theta}_\text{cond} = $($\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}, \boldsymbol{S}, \boldsymbol{b}_1, \boldsymbol{b}_2, ..., \boldsymbol{b}_n)$. This leads to the conditional data likelihood,

\begin{equation}
\label{eq : conditional_data_likelihood}
f(\boldsymbol{y}|\boldsymbol{\theta}_\text{cond}) = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i)
\end{equation}

Based on this conditional likelihood, \citet{celeux_deviance_2006} proposed the following DIC definition.

\begin{equation}
\label{eq : DIC6}
\text{DIC}_6 = -4\text{E}_{\boldsymbol{\theta}_\text{cond}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\theta}_{\text{cond}}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\theta}}_\text{cond})})
\end{equation}

where
$\boldsymbol{\hat{\theta}}_\text{cond} = \argmax_{\boldsymbol{\theta}_\text{cond}}p(\boldsymbol{\theta}_\text{cond}|\boldsymbol{y})$, and $\text{E}_{\boldsymbol{\theta}_\text{cond}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\theta}_{\text{cond}})}|\boldsymbol{y}])$ can be approximated as done in equation \ref{eq : mean_posterior_deviance_approx}.

\section{Marginal Likelihood}
\label{sec : marginal_likelihood}

The marginal likelihood of data represents the probablilty of data given the model. This can be calculated by margilizing the likelihood over the model parameters $\boldsymbol{\theta}$. 

\begin{equation}
\label{eq : marginal_likelihood}
p(\boldsymbol{y}|M) = \int p(\boldsymbol{y}|\boldsymbol{\theta}, M) p(\boldsymbol{\theta}|M) \diff \boldsymbol{\theta}
\end{equation}

Given two competeing models for the data, $M_1$ and $M_2$, one can further use \ref{eq : marginal_likelihood} to calculate the odds of model $M_1$ against the model $M_2$ given the data. i.e. Posterior odds. This ofcourse means that it is a comparative measure as $\sim M_1 = M_2$. One can write the posterior odds as

$$\frac {p(M_1|\boldsymbol{y})}{p(M_2|\boldsymbol{y})} = \frac {p(\boldsymbol{y}|M_1) p(M_1)} {p(\boldsymbol{y}|M_2) p(M_2)}$$

where $\frac {p(M_1)}{p(M_2)}$ is called prior odds, and $\frac {p(\boldsymbol{y}|M_1)} {p(\boldsymbol{y}|M_2)}$ is called the Bayes Factor.\\

Since we have the same prior belief in each of these models, prior odds is equal to 1. To calculate the Bayes Factor we will use the method proposed by \citet{chib_marginal_1995}. Chib's idea is that one can rewrite the Bayes rule in equation \ref{eq : bayes_rule} to get the marginal likelihood formula as

\begin{equation}
\label{eq : chib_trick}
m(\boldsymbol{y}) = p(\boldsymbol{y}|M) = \dfrac {L(\boldsymbol{\theta}|\boldsymbol{y}, M) p(\boldsymbol{\theta}|M)} {p(\boldsymbol{\theta}|\boldsymbol{y}, M)}
\end{equation}

Equation \ref{eq : chib_trick} is valid for all $\boldsymbol{\theta}$, though Chib recommends using posterior mode $\argmax_\theta p(\boldsymbol{\theta}|\boldsymbol{y})$ of parameters or the maximum likelihood estimate $\argmax_\theta L(\boldsymbol{\theta}|\boldsymbol{y})$. We decided to choose the latter of the two. Further, in context of the Bayesian heterogeneity model, we will denote the selected parameter values as ${\boldsymbol{\beta}}^*$, ${\sigma^2}^*$ and $\boldsymbol{\nu}^*$. Thus Chib's approximation for marginal likelihood on log scale is given by,

\begin{equation}
\label{eq : chib_approx}
\log{\hat{m}(\boldsymbol{y})} = \log{L({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})} + \log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*)} - \log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})}
\end{equation}

Note that we have dropped the model indicator $M$ from equation \ref{eq : chib_approx} for readability. We will now show calculations for determining the marginal likelihood value using Chib's approxmiation.\\ 

Firstly $\log{L({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})} = f(\boldsymbol{y}|{\boldsymbol{\beta}}^*, \sigma^2, {\boldsymbol{\nu}}^*)$ can be easily determined using the formula given in equation \ref{eq : obs_data_likelihood}. The calculation of $\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*)}$ is also straightforward because we take independent priors for these parameters, the details of which are given in section \ref{subsec : choice_priors}. Assuming that the parameters of component densities of the mixture distribution of random effects are independent, one can use the following to calculate $\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})}$.

\begin{multline}
\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})} = 
\sum_{k=1}^K{\log{p(G_k^*|\boldsymbol{y})}} + 
\sum_{k=1}^K{\log{p({\boldsymbol{b}_k^C}^*|G_k^*, \boldsymbol{y})}} + 
\log{p({\sigma^2}^*|G_k^*, {\boldsymbol{b}_k^C}^*, \boldsymbol{y})}\\
+ \log{p({\boldsymbol{\beta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*, \boldsymbol{y})} + 
\log{p({\boldsymbol{\eta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*,{\boldsymbol{\beta}}^*, \boldsymbol{y})}
\end{multline}

An interesting problem one faces in such an expansion is that the posteriors may not be available as a well known density. For e.g. we began with choosing indepdent gamma priors for precision parameters of random effects and uniform prior for correlation. However the posterior density was not well known. One could try to fit it with a wrapper density however as we will show ahead this could be practically infeasible. An obvious alternative is to choose conjugate priors in such situation. However as we mentioned in section \ref{subsec : choice_priors} the joint conjugate prior in the case of unknown mean and precision matrix is a Normal-Wishart-Prior and the joint posterior is a Normal-Wishart-Posterior. Although one does not use them in practice while using BUGS family of software, the problem of posterior being from an unknown family remains the same. \citet{chib_marginal_1995} suggested using the Rao-Blackwellization method to solve this problem. For e.g. the Rao-Blackwellized estimate of $p(G_k^*|\boldsymbol{y})$ is given by

\begin{equation}
\label{eq : rao_blackwellization_wishart}
\begin{split}
\prod_{k=1}^K p(G_k^*|\boldsymbol{y}) & = \int \prod_{k=1}^{K} p(G_k^*| \boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}, \boldsymbol{b}_k^C) p(\boldsymbol{b}_1^C, \boldsymbol{b}_2^C,..., \boldsymbol{b}_K^C,\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}) 
\diff{\boldsymbol{b}_1^C} \diff{\boldsymbol{b}_2^C} ... \diff{\boldsymbol{b}_K^C} \diff{\boldsymbol{b}} \diff{\boldsymbol{S}}\\
& \approx \frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} p(G_k^*| \boldsymbol{y}, \boldsymbol{b}^{(l)}, \boldsymbol{S}^{(l)}, {\boldsymbol{b}_k^C}^{(l)
})\\
& \approx \frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} f_{\mathcal{W}^{-1}}(G_k^*; n_k^{(l)}+n_0, \Psi + \sum_{i=1}^{n_k^{(l)}}(\boldsymbol{b_i}^{(l)} - {\boldsymbol{b}_k^C}^{(l)})(\boldsymbol{b_i}^{(l)} - {\boldsymbol{b}_k^C}^{(l)})^T)
\end{split}
\end{equation}

where, $n_k^{(l)}$ are number of subjects classified under component $k$ in iteration $l$ and $(n_0, \Psi)$ are the parameters for the inverse wishart distribution specified as prior for the variance covariance matrix of the component densities. In \ref{eq : rao_blackwellization_wishart} one approximates the integral with the samples obtained from the MCMC iterations. As we can see the benefit of this approach is that $p(G_k^*| \boldsymbol{y}, \boldsymbol{b}^{(l)}, \boldsymbol{S}^{(l)}, {\boldsymbol{b}_k^C}^{(l)})$ is the well known inverse wishart density. However in cases when this posterior is not well known, then given that the large number of MCMC iterations one does, it is not possible to manually check and fit wrapper densities to posterior densities. The use kernel density estimation procedures can also be dismissed as the posteriors may require approximation using different parametric families. It is because of these reasons we avoided calculation of Bayes factor in the case where we took indepdent gamma priors for precision of random effects and uniform prior for correlation.\\

Proceeding further with the Rao-Blackwellization procedure one can obtain the following approximations for the other parameters.

\begin{equation}
\prod_{k=1}^K p({\boldsymbol{b}_k^C}^*|G_k^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} f_N({\boldsymbol{b}_k^C}^*; (G_0^{-1} + n_k^{(l)} {G_k^*}^{-1})^{-1} (G_0^{-1}\boldsymbol{\mu}_0 + n_k^{(l)} {G_k^*}^{-1} \bar{\boldsymbol{b}_{ik}^{(l)}}) , (G_0^{-1} + n_k {G_k^*}^{-1})^{-1})
\end{equation}

\begin{equation}
p({\sigma^2}^*|G_k^*, {\boldsymbol{b}_k^C}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_{Inv-Gamma}({\sigma^2}^*; \alpha_0 + \frac {\sum_{i=1}^n m_i} 2, 
\beta_0 + \frac {\sum_{i=1}^n \sum_{j=1}^{m_i} (y_{ij} - \boldsymbol{x_{ij}}\boldsymbol{\beta}^{(l)} - \boldsymbol{z_{ij}}\boldsymbol{b_i^{(l)}})} 2)
\end{equation}

\begin{equation}
p({\boldsymbol{\beta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_N({\boldsymbol{\beta}}^*; (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{y} - {\boldsymbol{Zb}}^{(l)}), {\sigma^2}^*(\boldsymbol{X}^T\boldsymbol{X})^{-1})
\end{equation}

\begin{equation}
p({\boldsymbol{\eta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*,{\boldsymbol{\beta}}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_{Dir}({\boldsymbol{\eta}}^*; {a_0}_1 + n_1^{(l)}, {a_0}_2 + n_2^{(l)}, ..., {a_0}_K + n_K^{(l)})
\end{equation}

where, 
$(\boldsymbol{\mu}_0, G_0)$ are the parameters for the multivariate normal prior for the mean $\boldsymbol{b}_k^C$ of the $k^\text{th}$ component density,\\
$\bar{\boldsymbol{b}_{ik}^{(l)}} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) \boldsymbol{b_i^{(l)}}} {n_k^{(l)}}$ is the mean of the estimated random effects corresponding to the $n_k$ subjects classified under the $k^\text{th}$ component in the $l^\text{th}$ MCMC iteration,\\
$(\alpha_0, \beta_0)$ are the parameters of the inverse gamma density specified as the prior for the within subject variance $\sigma^2$,\\
${a_0}_1, {a_0}_2,..., {a_0}_K$ are the parameters of the Dirichlet density specified as the prior for component weight vector $\boldsymbol{\eta}$.\\

Using these values an estimate of $\log{p({\boldsymbol{\beta}}^*, {\sigma^2}^*, \boldsymbol{\nu}^*|\boldsymbol{y})}$ is available which can be further substituted in equation \ref{eq : chib_approx} to obtain $\log{\hat{m}(\boldsymbol{y})}$. In ideal cases, i.e. where marginal likelihood is known to work well as a model selection criteria, models with higher value of $\log{\hat{m}(\boldsymbol{y})}$ should be chosen.

\section{Posterior predictive checks}
\label{sec : ppc}
The motivation behind a posterior predictive check is to evaluate the model fit using simulations from the posterior predictive distribution(PPD) $p(\boldsymbol{\tilde{y}}|\boldsymbol{y})$. For a simple model such as $y_i = \mu + \varepsilon_i, \varepsilon_i \sim N(0, \sigma^2)$, one could do a quick an informal check by sampling 1000 values from the PPD 20 times and make 20 histograms to show the density. If the histograms do not match with the histogram of the original sample one could say that the model did not fit the data well.\\

In context of mixture distributions \citet{fruhwirth-schnatter_finite_2013} suggest that one should first sample allocation vector $\boldsymbol{\tilde{S}}$ based on the posterior density of the weight distribution $p(\boldsymbol{\eta}|\boldsymbol{y})$ and then generate samples from $p(\boldsymbol{\tilde{y}}|\boldsymbol{\tilde{S}}, [\boldsymbol{\theta}|\boldsymbol{y}])$. A similar approach can be followed in hierarchical models by generate new random effects $\boldsymbol{\tilde{b}}$. \citet{marshall_approximate_2003} termed this as a mixed predictive approach. Although they suggest a cross validatory method where $\boldsymbol{\tilde{b}}|\boldsymbol{y_{(i)}}$, we will not use it as it requires a significant computational effort and as we will see ahead we will obtain a useful PPC. The alternative to mixed predictive approach is to use the posterior allocations $p(\boldsymbol{b}|\boldsymbol{y}$ instead of $\boldsymbol{\tilde{b}}$, however the problem with this approach is that future values $\boldsymbol{\tilde{y}}$ are influenced by $\boldsymbol{y}$ not only through $\boldsymbol{\theta}$ but also through subject specific effects $\boldsymbol{b}$. Since the subject specific effects are full bayesian estimates generated from the observed data, such an approach leads to a more conservative posterior predictive check \citep{congdon_applied_2010}.\\

\subsection{PPC for the Bayesian heterogeneity model}
\label{subsec : ppc_bhtge}
\subsubsection{Detecting overfitting the number of components}
When more than the required number of components are fitted, during certain iterations some of the components remain empty. In such cases, the posterior samples of variance covariance matrix $G_k$ as well as of mean $\boldsymbol{b}_k^C$ are taken from the prior. Thus the posterior samples of such components are randomly large or small values. Thus if we manage to sample new random effects $\boldsymbol{\tilde{b}}$ from these distributions, we will observe them to be very large or very small.\\

Unfortunately there are two quirks in this approach. Firstly, the weight components $\eta_k$ of empty components is sampled from the Dirichlet prior which gets information from all other components as well. Thus such components get extremely small weights from the posterior sample. One could handle this case by using a very large posterior predictive sample. The second problem is that if one uses a Wishart prior for the variance covariance matrix of component densities then although it is non-informative for the correlation, it imposes restrictions on the variances. One can thus choose independent gamma priors for variances and uniform prior for correlation in such a scenario, albeit under the restriction that the sample variance covariance matrix is invertible.\\

The interesting side of the above mentioned approach is that it is only possible with mixed predictive checking. If one checks the posterior samples of $\boldsymbol{b}_i$ then they will find that they do not show any anamoly under over/underfitting. Thus making the classical approach conservative. However once again the the unique problem with the Bayesian heterogenity model in the mixed predictive approach is that because of the new allocation of $\boldsymbol{\tilde{S}}$, one cannot reuse the observed design matrix of the data, i.e reusing $\boldsymbol{X}$ for the new subjects. One solution for this problem is avoiding the $\boldsymbol{X\beta}$ part altogether by subtracting it from the observed and the posterior predicted data. The benefit of this approach is that even in cases where the random effect structure is misspecified the fixed effect estimates are unbiased. Thus we can obtain a test statistic using $\boldsymbol{r} = \boldsymbol{y} - \boldsymbol{X}[\boldsymbol{\beta}|\boldsymbol{y}]$, i.e. based on the observed data and then compare it with a test statistic measure based on $\boldsymbol{\tilde{r}}=\boldsymbol{Z\tilde{b}} + \boldsymbol{\varepsilon}$. One such test statistic could be

\begin{equation}
\label{eq : ppc_test_statistic}
T(\boldsymbol{r}) = \frac 1 {\sum_{i=1}^n m_i} \sum_{i=1}^n \sum_{j=1}^{m_i} {(r_{ij}-\bar{r}_{i.})}^2
\end{equation}

\subsubsection{Underfitting the number of components}
The Bayesian heterogeneity model poses a unique problem for detecting underfitting via PPC. As mentioned in the previous section, underfitted models perform as good as models with the right number of components. The reason is that the full bayesian estimates for random effects $\boldsymbol{b}_i$ can be more or less the same for underfitted as well as rightly and overfitted models. This further results into an almost equal estimate of within subject variance $\sigma^2$ for the various models. The mean structure parameter estimates are also almost the same and thus eventually it is very difficult to differentiate between the models using PPC. One way to solve this issue is to detect first the minimum number of components needed to enforce overfitting using the approach in the previous subsection, and then the choice among the remaining models could be done on the basis of interpretation of the clusters and the size of the clusters.
 
\subsection{Posterior predictive p-values}
The motivation of Posterior predictive p-values(PPP) is similar to frequentist p-values, albeit avaraged over the entire posterior distribution of the test stastistic. Given a test statistic $T(\boldsymbol{y})$, frequentist p-values check the probablity $P(T(\boldsymbol{\tilde{y}}) > T(\boldsymbol{y})$, where $p(\boldsymbol{\tilde{y}}) = p(\boldsymbol{y}|\boldsymbol{\hat{\theta}})$. In the bayesian paradigm the parameter $\boldsymbol{\theta}$ has a posterior distribution and so we find the same probability as before but average it over the entire posterior $p(\theta|\boldsymbol{y})$. A small PPP value indicates bad fit of model to the data. In context of the Bayesian heterogeneity model PPP values may not be as desirable because models with underfitting/overfitting number of components for the mixture, all fit the sample very well and only overfitted models can be easily detected. However they can still be used to detect a poor fit to the data itself.
