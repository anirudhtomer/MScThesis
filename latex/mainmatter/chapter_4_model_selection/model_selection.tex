% !TEX root =  ../../thesis.tex
\chapter{Model selection criteria}
\label{ch : model_selection}

In practice the right number of components in a mixture distribution are not known in advance. In this chapter we have described 3 of the Bayesian model selection criteria to select the model with the right number of mixture components in a Bayesian heterogeneity model.

\section{Deviance information criteria}
\label{sec : dic}

The Deviance information criteria or DIC was first proposed by \citet{spiegelhalter_bayesian_2002} for Bayesian model selection. The motivation for DIC is similar to frequentist AIC/BIC criteria in the sense that DIC also penalizes more elaborate models using a penalty component. The definition for DIC is given by 

$$\text{DIC} = -2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})} + 2\text{p}_\text{D}$$

where $\boldsymbol{\bar{\theta}} = \int \boldsymbol{\theta} p(\boldsymbol{\theta}|\boldsymbol{y}) \diff\boldsymbol{\theta}$ is the posterior mean of the parameters. The penalty for model complexity is given by 
$\text{p}_\text{D} = -2\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|\boldsymbol{\theta})} + \log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})})$. It can also be written as,

$$\text{p}_\text{D}=\overline{D(\boldsymbol{\theta})} - D(\boldsymbol{\bar{\theta}})$$

where $\overline{D(\boldsymbol{\theta})}$ is the posterior mean of Bayesian deviance $D(\boldsymbol{\theta})$, and $D(\boldsymbol{\bar{\theta}})$ is the Bayesian deviance evaluated at the posterior mean of the parameters. Bayesian deviance is defined as $D(\boldsymbol{\theta}) = -2\log{p(\boldsymbol{y}|\boldsymbol{\theta})} + 2\log{f(\boldsymbol{y})}$. We do not discuss the term $f(\boldsymbol{y})$ because it cancels out in the expression for $\text{p}_\text{D}$.

\subsection{DIC for missing data models}
\label{subsec : DIC_missing_data_models}
Mixture models and mixed models are both a member of the class of models called missing data models. The allocation vector $\boldsymbol{S}$ in a mixture model, and the matrix of random effects $\boldsymbol{b}=(\boldsymbol{b}_1^T, \boldsymbol{b}_1^T, ..., \boldsymbol{b}_n^T)^T$ in a LMM, are not observed directly. Thus one could have various incompatible definitions of DIC based on observed data likelihood, complete data likelihood and conditional data likelihood, as shown by DeIorio and Robert in a discussion on the paper of \citet{spiegelhalter_bayesian_2002}. Further, \citet{celeux_deviance_2006} proposed multiple definitions of DIC under each of the aforementioned likelihood classes and showed that each has a different value and thus a different impact on model selection. In this thesis we took some of those definitions and applied them on the Bayesian heterogeneity model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Observed Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Observed DIC}
Let us denote the set of all the unknown parameters in the Bayesian heterogeneity model as $\boldsymbol{\theta}$. i.e. $\boldsymbol{\theta} = (\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu})$, where $\boldsymbol{\nu}$ is as defined in section \ref{subsec : bhtge}. The first category of DIC is associated with observed data likelihood $f(\boldsymbol{y}|\boldsymbol{\theta})$ which can be obtained by marginalizing over the allocation $S_i$ and random effects $\boldsymbol{b}_i$ for the subjects. The following expression gives the observed data likelihood for the Bayesian heterogeneity model.

\begin{equation}
\label{eq : obs_data_likelihood}
p(\boldsymbol{y}|\boldsymbol{\theta}) = \prod_{i=1}^n \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_k^C, \boldsymbol{Z}_{i} G_k \boldsymbol{Z}_{i}^T+ R_i) \eta_k
\end{equation}
 
where $f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_k^C, \boldsymbol{Z}_{i} G_k \boldsymbol{Z}_{i}^T+ R_i)$ denotes the density of the normal $N(\boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_k^C, \boldsymbol{Z}_{i} G_k \boldsymbol{Z}_{i}^T+ R_i)$ distribution. Based on equation \ref{eq : obs_data_likelihood} we extended the various definitions of DIC proposed by \citet{celeux_deviance_2006}. The first one is the classical definition for DIC, as shown below.

\begin{equation}
\label{eq : DIC1}
\text{DIC}_1 = -4\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|\boldsymbol{\theta})}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})})
\end{equation}

One of the problems with $\text{DIC}_1$ is that in case of label switching, one may obtain a negative $\text{p}_\text{D}$. In our simulation study we encountered this problem as well (section \ref{subsec : dic_simulation_results}). The reason is that when label switching happens, the mean of the posteriors may take a value which lies somewhere in between the various modes. On the other hand, in such a scenario a posterior mode will belong to only one of the modal regions. Thus the next definition of DIC is formed by replacing posterior mean with the posterior mode in the calculation of $D(\boldsymbol{\bar{\theta}})$.

\begin{equation}
\label{eq : DIC2}
\text{DIC}_2 = -4\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|\boldsymbol{\theta})}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\theta}}_M)})
\end{equation}

where
$\boldsymbol{\hat{\theta}_M} = \argmax_{\boldsymbol{\theta}}p(\boldsymbol{\theta}|\boldsymbol{y})$ is the posterior mode of the parameters. \citet{celeux_deviance_2006} further suggest that for models where non identifiability of parameters is endemic, as in the case of mixtures, one should use the following MCMC estimator for the approximation of $p(\boldsymbol{y}|\boldsymbol{\theta})$,

$$\hat{p}(\boldsymbol{y}) = \prod_{i=1}^n \hat{p}(\boldsymbol{y}_i) = \prod_{i=1}^n \frac 1 m \sum_{l=1}^m \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta}^{(l)} + \boldsymbol{Z}_i {\boldsymbol{b}_k^C}^{(l)}, \boldsymbol{Z}_{i} G_k^{(l)} \boldsymbol{Z}_{i}^T+ R_i^{(l)}) \eta_k^{(l)}$$

where $\boldsymbol{\beta}^{(l)}, {\boldsymbol{b}_k^C}^{(l)}, G_k^{(l)}, R_i^{(l)}, \eta_k^{(l)}$ are the samples from the $l^{\text{th}}$ MCMC iteration of a chain of length $m$, and $\hat{p}(\boldsymbol{y})$ is the MCMC estimator for $p(\boldsymbol{y}|\boldsymbol{\theta})$. The estimator is also called the MCMC predictive density estimator, and gives us the next definition of DIC.

\begin{equation}
\label{eq : DIC3}
\text{DIC}_3 =-4\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|\boldsymbol{\theta})}) + 2\log{\hat{p}(\boldsymbol{y})}
\end{equation}

The benefit of using $\text{DIC}_3$ is that $\text{p}_\text{D}$ always takes a positive value. While doing the simulation study we found that this claim was not only true, but $\text{p}_\text{D}$ also never took extreme values; even when models were severely overfitted leading to label switching. Lastly, in each of the equations \ref{eq : DIC1}, \ref{eq : DIC2}, \ref{eq : DIC3},  $\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|\boldsymbol{\theta})}) $ can be calculated by using the following approximation.

\begin{equation}
\label{eq : mean_posterior_deviance_approx}
\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|\boldsymbol{\theta})})  = \frac 1 m \sum_{l=1}^m\log{p(\boldsymbol{y}|\boldsymbol{\theta}^{(l)})}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Complete Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Complete DIC}
The second class of the DIC is based on the complete data likelihood. The complete data for the $i^\text{th}$ subject in a Bayesian heterogeneity model is based on $(\boldsymbol{y}_i, S_i, \boldsymbol{b}_i)$. For the $i^\text{th}$ subject we will denote the complete data by $\boldsymbol{y}_i^F$ and for the entire data set we will denote it by $\boldsymbol{y}^F$. The following equation shows the complete data likelihood for the Bayesian heterogeneity model.

\begin{equation}
\label{eq : complete_data_likelihood}
\begin{split}
f(\boldsymbol{y}^F | \boldsymbol{\theta}) & = \prod_{i=1}^n f(\boldsymbol{y}_i|\boldsymbol{b}_i, S_i) f(\boldsymbol{b}_i|S_i) f(S_i)\\
& = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i) f_N(\boldsymbol{b}_i; \boldsymbol{b}_{S_i}^C, G_{S_i}) f(S_i; \boldsymbol{\eta})
\end{split}
\end{equation}

Based on the complete data $\boldsymbol{y}^F$, the complete data DIC is given by following expression,

\begin{equation}
\label{eq : complete_data_dic}
\text{DIC} = -4\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}^F}(\log{p(\boldsymbol{y}^F|\boldsymbol{\theta})}) + 
2\log{p(\boldsymbol{y}^F|\boldsymbol{\bar{\theta}})})
\end{equation}

Since part of the complete data are not observed, \citet{celeux_deviance_2006} propose integrating the expression in \ref{eq : complete_data_dic} with respect to the missing data $(\boldsymbol{b}, \boldsymbol{S})$ to obtain the following definition of DIC.

\begin{equation}
\label{eq : DIC4}
\text{DIC}_4 = -4\text{E}_{\boldsymbol{\theta}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}^F|\boldsymbol{\theta})}) + 
2\text{E}_{\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}^F|\boldsymbol{\bar{\theta}})})
\end{equation}

The first part of $\text{DIC}_4$ is not available in closed form for the Bayesian heterogeneity model, however it can still be approximated using the output of Gibbs sampler in the same way as in equation \ref{eq : mean_posterior_deviance_approx}. The motivation for this approximation is that during each iteration of the Gibbs sampler, parameter values are sampled from the conditional distribution of the parameters. i.e. in our case conditional on the other parameters and unobserved data both. We further verified the approximation by comparing the computed $\text{DIC}_4$ using the closed form solution for a mixture distribution, with the aforementioned approximation, both suggested by \citet{celeux_deviance_2006}. We found that the two computed DIC values differ only in the decimal places.\\

While the second part of $\text{DIC}_4$, i.e. $\text{E}_{\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}^F|\boldsymbol{\bar{\theta}})})$ can also be approximated in the same way as in equation \ref{eq : mean_posterior_deviance_approx}, but the added complexity here is that the posterior mean of the parameters is given by $\boldsymbol{\bar{\theta}} = \int \boldsymbol{\theta} p(\boldsymbol{\theta}|\boldsymbol{y}^F) \diff\boldsymbol{\theta}$. This is because $\text{DIC}_4$ is based on the complete data $\boldsymbol{y}^F$ and the MCMC sampler only returns the posterior density $p(\boldsymbol{\theta}|\boldsymbol{y})$, whereas we require $p(\boldsymbol{\theta}|\boldsymbol{y}^F)$. \citet{celeux_deviance_2006} suggested using the posterior estimates of $\boldsymbol{b}, \boldsymbol{S}$ from $p(\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y})$ to analytically calculate the posterior distribution of parameters $p(\boldsymbol{\theta}|\boldsymbol{y}^F)$, from which the posterior means of the various parameters can be obtained as shown below.

$$\boldsymbol{\bar{\beta}}^{(l)} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{Zb^{(l)}})$$
$$\bar{\sigma^2}^{(l)} = \frac {(\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})^T (\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})} {(\sum_{i=1}^n m_i - p - 1) -2}$$
$$\bar{\boldsymbol{b}_k^C}^{(l)} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) \boldsymbol{b_i^{(l)}}} {n_k^{(l)}}$$
$$\bar{G_k^{(l)}} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) (\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})(\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})^T} 
{(n_k^{(l)} - 1) - \text{rank}(\bar{\boldsymbol{b}_k^C}^{(l)}) - 1}$$
$$\bar{\eta}_k^{(l)} = \frac {a_k + n_k^{(l)}} {\sum_{u=1}^K a_u + n}$$

where, $\boldsymbol{\bar{\beta}}^{(l)}$, $\bar{\sigma^2}^{(l)}$ and $\boldsymbol{\bar{\nu}}^{(l)}$ are the expected values of parameters of interest at the $l^\text{th}$ iteration. These can be further used to approximate $\text{E}_{\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}^F|\boldsymbol{\bar{\theta}})})$.\\

The next definition of DIC under the class of complete data is motivated by the fact that the at times $\text{E}(\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y})$ takes values outside the support of the joint distribution of $\boldsymbol{b}, \boldsymbol{S}$ \citep{celeux_deviance_2006}. Thus the following definition of DIC is proposed using the posterior mode as the estimate instead.

\begin{equation}
\label{eq : DIC5}
\text{DIC}_5 = -4\text{E}_{\boldsymbol{\theta}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}^F|\boldsymbol{\theta})}) + 
2\log{p(\boldsymbol{y}, \boldsymbol{\hat{b}}_M, \boldsymbol{\hat{S}}_M|\boldsymbol{\hat{\theta}}_M)}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conditional Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Conditional DIC}
The third class of the DIC is based on the assumption that $\boldsymbol{b}, \boldsymbol{S}$ can be seen as an additional parameter rather than as missing data. We will represent the new posterior parameter space as $\boldsymbol{\theta}^\text{cond} = (\boldsymbol{\theta}, \boldsymbol{b}, \boldsymbol{S})$. The following equation shows the conditional data likelihood,

\begin{equation}
\label{eq : conditional_data_likelihood}
p(\boldsymbol{y}|\boldsymbol{\theta}^\text{cond}) = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i)
\end{equation}

Based on this conditional likelihood, \citet{celeux_deviance_2006} proposed the following DIC definition.

\begin{equation}
\label{eq : DIC6}
\text{DIC}_6 = -4\text{E}_{\boldsymbol{\theta}^\text{cond}} (\log{p(\boldsymbol{y}|\boldsymbol{\theta}^\text{cond})}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\theta}}^\text{cond}_M)})
\end{equation}

where
$\boldsymbol{\hat{\theta}}^\text{cond}_M = \argmax_{\boldsymbol{\theta}^\text{cond}}p(\boldsymbol{\theta}^\text{cond}|\boldsymbol{y})$, and $\text{E}_{\boldsymbol{\theta}^\text{cond}} (\log{p(\boldsymbol{y}|\boldsymbol{\theta})})$ can be approximated as done in equation \ref{eq : mean_posterior_deviance_approx}.

\section{Marginal Likelihood}
\label{sec : marginal_likelihood}

The marginal likelihood of the data represents the probability of data given the model. This can be calculated by marginalizing the likelihood over the model parameters $\boldsymbol{\theta}$. 

\begin{equation}
\label{eq : marginal_likelihood}
p(\boldsymbol{y}|M) = \int p(\boldsymbol{y}|\boldsymbol{\theta}, M) p(\boldsymbol{\theta}|M) \diff \boldsymbol{\theta}
\end{equation}

Given two competing models for the data, $M_1$ and $M_2$, one can further use expression \ref{eq : marginal_likelihood} to calculate the odds of model $M_1$ against the model $M_2$ given the data. These odds are called the Posterior odds and can be written as,

$$\frac {p(M_1|\boldsymbol{y})}{p(M_2|\boldsymbol{y})} = \frac {p(\boldsymbol{y}|M_1) p(M_1)} {p(\boldsymbol{y}|M_2) p(M_2)}$$

where $\frac {p(M_1)}{p(M_2)}$ is called prior odds, and $\frac {p(\boldsymbol{y}|M_1)} {p(\boldsymbol{y}|M_2)}$ is called the Bayes Factor. Since we have the same prior belief in each of these models, prior odds is equal to 1. Marginal likelihood is a relative measure of model fit and unless prior odds are not equal to 1, the marginal likelihood will be equal to Bayes Factor. To calculate the Bayes Factor we will use the method proposed by \citet{chib_marginal_1995}. Chib's idea is that one can rewrite the Bayes rule in equation \ref{eq : bayes_rule} to get the marginal likelihood formula as

\begin{equation}
\label{eq : chib_trick}
m(\boldsymbol{y}) = p(\boldsymbol{y}|M) = \dfrac {L(\boldsymbol{\theta}|\boldsymbol{y}, M) p(\boldsymbol{\theta}|M)} {p(\boldsymbol{\theta}|\boldsymbol{y}, M)}
\end{equation}

Equation \ref{eq : chib_trick} is valid for all $\boldsymbol{\theta}$, though Chib recommends using posterior mode $\argmax_\theta p(\boldsymbol{\theta}|\boldsymbol{y})$ of parameters or the maximum likelihood estimate $\argmax_\theta L(\boldsymbol{\theta}|\boldsymbol{y})$. We decided to choose the latter of the two. Further, in context of the Bayesian heterogeneity model, we will denote the selected parameter value as 
$\boldsymbol{\theta}^*=(\boldsymbol{\beta}^*, {\sigma^2}^*, \boldsymbol{\nu}^*)$. Thus Chib's approximation for marginal likelihood on log scale is given by,

\begin{equation}
\label{eq : chib_approx}
\log{\hat{m}(\boldsymbol{y})} = \log{L(\boldsymbol{\theta}^*|\boldsymbol{y})} + \log{p(\boldsymbol{\theta}^*)} - \log{p(\boldsymbol{\theta}^*|\boldsymbol{y})}
\end{equation}

Note that we have dropped the model indicator $M$ from equation \ref{eq : chib_approx} for readability. We will now show calculations for determining the marginal likelihood value using Chib's approximation.\\ 

Firstly $\log{L(\boldsymbol{\theta}^*|\boldsymbol{y})} = p(\boldsymbol{y}|\boldsymbol{\theta}^*)$ can be easily determined using the formula given in equation \ref{eq : obs_data_likelihood}. The calculation of $\log{p(\boldsymbol{\theta}^*)}$ is also straightforward because we take independent priors for the parameters in our model, the details of which are given in section \ref{subsec : choice_priors}. Assuming that the parameters of component densities of the mixture distribution of random effects are independent, one can use the following to calculate $\log{p(\boldsymbol{\theta}^*|\boldsymbol{y})}$.

\begin{multline}
\label{eq : chib_approx_posterior_part}
\log{p(\boldsymbol{\theta}^*|\boldsymbol{y})} = 
\sum_{k=1}^K{\log{p(G_k^*|\boldsymbol{y})}} + 
\sum_{k=1}^K{\log{p({\boldsymbol{b}_k^C}^*|G_k^*, \boldsymbol{y})}} + 
\log{p({\sigma^2}^*|G_k^*, {\boldsymbol{b}_k^C}^*, \boldsymbol{y})}\\
+ \log{p({\boldsymbol{\beta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*, \boldsymbol{y})} + 
\log{p({\boldsymbol{\eta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*,{\boldsymbol{\beta}}^*, \boldsymbol{y})}
\end{multline}

An interesting problem one faces in such an expansion is that the posteriors may not be available as a well known density. For e.g. in the case of covariance matrix $G_k$, an inverse Wishart prior for the covariance matrix $G_k$ is a conjugate prior, conditional on the true mean $\boldsymbol{b}_k^C$ being known. To solve this problem, \citet{chib_marginal_1995} suggested using the Rao-Blackwellization method. For e.g. the Rao-Blackwellized estimate of $p(G_k^*|\boldsymbol{y})$ is given by,

\begin{equation}
\label{eq : rao_blackwellization_wishart}
\begin{split}
\prod_{k=1}^K p(G_k^*|\boldsymbol{y}) & = \int \prod_{k=1}^{K} p(G_k^*| \boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}, \boldsymbol{b}_k^C) p(\boldsymbol{b}_1^C, \boldsymbol{b}_2^C,..., \boldsymbol{b}_K^C,\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}) 
\diff{\boldsymbol{b}_1^C} \diff{\boldsymbol{b}_2^C} ... \diff{\boldsymbol{b}_K^C} \diff{\boldsymbol{b}} \diff{\boldsymbol{S}}\\
& \approx \frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} p(G_k^*| \boldsymbol{y}, \boldsymbol{b}^{(l)}, \boldsymbol{S}^{(l)}, {\boldsymbol{b}_k^C}^{(l)
})\\
& \approx \frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} f_{\mathcal{W}^{-1}}(G_k^*; n_k^{(l)}+n_0, \Psi + \sum_{i=1}^{n_k^{(l)}}(\boldsymbol{b_i}^{(l)} - {\boldsymbol{b}_k^C}^{(l)})(\boldsymbol{b_i}^{(l)} - {\boldsymbol{b}_k^C}^{(l)})^T)
\end{split}
\end{equation}

where, $n_k^{(l)}$ are number of subjects classified under component $k$ in iteration $l$, $f_{\mathcal{W}^{-1}}$ represents the density function of an inverse Wishart distribution and $(n_0, \Psi)$ are the parameters for the inverse Wishart distribution specified as prior for the variance covariance matrix of the component densities. In \ref{eq : rao_blackwellization_wishart} one approximates the integral with the samples obtained from the MCMC iterations. As we can see the benefit of this approach is that $p(G_k^*| \boldsymbol{y}, \boldsymbol{b}^{(l)}, \boldsymbol{S}^{(l)}, {\boldsymbol{b}_k^C}^{(l)})$ is the well known inverse Wishart density. If instead of an inverse Wishart prior, independent inverse gamma priors for variance parameters of random effects and a uniform prior for correlation was chosen, then an interesting problem arises. The posterior distribution of correlation conditional on knowing the variance parameters, mean and complete data, is not available as a well known density in the Rao-Blackwellization procedure. One way to solve this problem is to approximate the posterior density with a well known density. However this is required to be done as many times as the number of MCMC iterations, making it practically infeasible. We thus avoided the calculation of Bayes factor in the case where we took independent inverse gamma priors for variance of random effects and uniform prior for correlation.\\

Proceeding further with the Rao-Blackwellization procedure we obtained the following approximations for the remaining parameters.

\begin{equation}
\prod_{k=1}^K p({\boldsymbol{b}_k^C}^*|G_k^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m \prod_{k=1}^{K} f_N({\boldsymbol{b}_k^C}^*; (G_0^{-1} + n_k^{(l)} {G_k^*}^{-1})^{-1} (G_0^{-1}\boldsymbol{\mu}_0 + n_k^{(l)} {G_k^*}^{-1} \bar{\boldsymbol{b}_{ik}^{(l)}}) , (G_0^{-1} + n_k {G_k^*}^{-1})^{-1})
\end{equation}

\begin{equation}
p({\sigma^2}^*|G_k^*, {\boldsymbol{b}_k^C}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_{Inv-Gamma}({\sigma^2}^*; \alpha_0 + \frac {\sum_{i=1}^n m_i} 2, 
\beta_0 + \frac {\sum_{i=1}^n \sum_{j=1}^{m_i} (y_{ij} - \boldsymbol{x_{ij}}\boldsymbol{\beta}^{(l)} - \boldsymbol{z_{ij}}\boldsymbol{b_i^{(l)}})} 2)
\end{equation}

\begin{equation}
p({\boldsymbol{\beta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_N({\boldsymbol{\beta}}^*; (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{y} - {\boldsymbol{Zb}}^{(l)}), {\sigma^2}^*(\boldsymbol{X}^T\boldsymbol{X})^{-1})
\end{equation}

\begin{equation}
p({\boldsymbol{\eta}}^*|G_k^*, {\boldsymbol{b}_k^C}^*, {\sigma^2}^*,{\boldsymbol{\beta}}^*, \boldsymbol{y}) \approx 
\frac 1 m \sum_{l=1}^m f_{Dir}({\boldsymbol{\eta}}^*; {a_0}_1 + n_1^{(l)}, {a_0}_2 + n_2^{(l)}, ..., {a_0}_K + n_K^{(l)})
\end{equation}

where, 
$(\boldsymbol{\mu}_0, G_0)$ are the parameters for the multivariate normal prior for the mean $\boldsymbol{b}_k^C$ of the $k^\text{th}$ component density,\\
$\bar{\boldsymbol{b}_{ik}^{(l)}} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) \boldsymbol{b_i^{(l)}}} {n_k^{(l)}}$ is the mean of the estimated random effects corresponding to the $n_k$ subjects classified under the $k^\text{th}$ component in the $l^\text{th}$ MCMC iteration,\\
$(\alpha_0, \beta_0)$ are the parameters of the inverse gamma density specified as the prior for the within subject variance $\sigma^2$,\\
${a_0}_1, {a_0}_2,..., {a_0}_K$ are the parameters of the Dirichlet density specified as the prior for component weight vector $\boldsymbol{\eta}$,\\
$f_{Inv-Gamma}$ and $f_{Dir}$ are the density functions of Inverse gamma and Dirichlet distributions respectively.\\

These values can be substituted in equation \ref{eq : chib_approx_posterior_part} to obtain an estimate of $\log{p(\boldsymbol{\theta}^*|\boldsymbol{y})}$, which can be further substituted in equation \ref{eq : chib_approx} to obtain $\log{\hat{m}(\boldsymbol{y})}$. In models where marginal likelihood is known to work well as a model selection criteria, models with higher value of $\log{\hat{m}(\boldsymbol{y})}$ should be preferred.

\section{Posterior predictive checks}
\label{sec : ppc}
The motivation behind a posterior predictive check is to evaluate the model fit using simulations from the posterior predictive distribution(PPD) $p(\boldsymbol{\tilde{y}}|\boldsymbol{y})$. For a simple model such as $y_i = \mu + \varepsilon_i, \varepsilon_i \sim N(0, \sigma^2)$, one can perform an informal check by sampling 1000(say) values from the PPD 20(say) times and then comparing the histogram of the original sample with the histogram of the data sampled from PPD. If the histograms do not match then one can say that the model does not fit the data well.\\

In context of mixture distributions \citet{fruhwirth-schnatter_finite_2013} suggest that one should first sample allocation vector $\boldsymbol{\tilde{S}}$ based on the posterior density of the weight distribution $p(\boldsymbol{\eta}|\boldsymbol{y})$ and then generate samples $\boldsymbol{\tilde{y}}$ from $p(\boldsymbol{\tilde{y}}|\boldsymbol{\tilde{S}}, \boldsymbol{\theta})$. A similar approach can be followed in hierarchical models by first generating new random effects $\boldsymbol{\tilde{b}}$. This was termed as the mixed predictive approach by \citet{marshall_approximate_2003}. As an alternative of mixed predictive approach, one can use the posterior allocations $\boldsymbol{b}|\boldsymbol{y}$ instead of $\boldsymbol{\tilde{b}}$. However the problem with this approach is that it leads to a more conservative posterior predictive check \citep{congdon_applied_2010}.\\

\subsection{PPC for the Bayesian heterogeneity model}
\label{subsec : ppc_bhtge}
\subsubsection{Detecting overfitting the number of components}
When more than the required number of components are fitted to the mixture distribution of random effects then  during the MCMC procedure some of the components may remain empty. i.e. no observation gets allocated to some of the components. In such a case, the posterior of variance covariance matrix $G_k$ as well as of mean $\boldsymbol{b}_k^C$ is sampled from the prior distribution. Since the priors we considered were non informative, the posterior samples of parameters are arbitrarily large or small values. Thus the random effects $\boldsymbol{\tilde{b}}$ sampled from the posterior distribution of the parameters of empty components, and the posterior predictive sample from $p(\boldsymbol{\tilde{y}} | \boldsymbol{\tilde{b}})$ will also be arbitrarily small or large. We used this property to form a posterior predictive check. However this approach is only possible with mixed predictive checking. The reason is that in our simulation study we found that the posterior samples from $p(\boldsymbol{b}_i|\boldsymbol{y})$ do not show any anomaly even if the mixture distribution of random effects is overfitted/underfitted. Thus following the classical PPC one may not be able to detect over/under fitting based on the posterior samples from $p(\boldsymbol{b}_i|\boldsymbol{y})$.\\

We will now discuss two of the issues with the above approach and their solutions. Firstly, the weights $\eta_k$ of empty components are sampled from the Dirichlet posterior which gets information from all other components as well. Thus iterations in which the components remain empty, the weights also remain small for the corresponding components. This problem can be obviated by sampling a large number of observations from the posterior predictive distribution. The second problem is that if one uses an inverse Wishart prior for the variance covariance matrix of component densities then although it is non-informative for the correlation, it imposes certain restrictions on the variances. One can instead choose independent inverse gamma priors for variances and uniform prior for correlation in such a scenario, albeit under the restriction that the sample variance covariance matrix is invertible.\\

 The first step in the mixed predictive check approach for Bayesian heterogeneity model is to sample the new allocations $\boldsymbol{\tilde{S}}$. The second step is to sample the random effects from $p(\boldsymbol{\tilde{b}}_i|\tilde{S}_i)$ for the subjects. We used the following test statistic based on these sampled random effects.

\begin{equation}
\label{eq : ppc_test_statistic}
T(\boldsymbol{r}) = \frac 1 {\sum_{i=1}^n m_i} \sum_{i=1}^n \sum_{j=1}^{m_i} {(r_{ij}-\bar{r}_{i.})}^2
\end{equation}

where, $r_{ij} = \boldsymbol{z}_{ij}\boldsymbol{\tilde{b}}_i + \varepsilon_{ij}$ for the mixed predictive distribution of the random effects and\\ $\bar{r}_{i.} = \frac 1 {m_i} \sum_{j=1}^{m_i} r_{ij}$. $\boldsymbol{z}_{ij}$ is as defined in section \ref{subsec : lmm_definition}. For the observed data set, $r_{ij}$ can be calculated as $r_{ij} = y_{ij} - \boldsymbol{x}_{ij}\boldsymbol{\beta}$, where $\boldsymbol{\beta}$ is sampled from the posterior distribution $p(\boldsymbol{\beta}|\boldsymbol{y})$. When the number of components in the mixture distribution of random effects are more than required, then during some of the iterations the test statistic is expected to have arbitrarily large values.

\subsubsection{Underfitting the number of components}
We found in our simulation study that it was not possible to distinguish the distribution of the test statistic $T(\boldsymbol{r})$ with underfitted mixture of random effects from the distribution of the test statistic with rightly fitted mixture of random effects. The reason for it is that the full Bayesian estimates for random effects $\boldsymbol{b}_i$ can be almost the same for underfitted as well as rightly and overfitted models. This further results into an almost equal estimate of within subject variance $\sigma^2$ for the various models. The mean structure parameter estimates are also almost the same and thus eventually with these minor differences it is impossible to differentiate between the models using PPC. One way to circumvent this issue is to fit as many components as possible till overfitting is detected, and then the choice among the underfitted models could be done on the basis of interpretation of the clusters and the size of the clusters.
 
\subsection{Posterior predictive p-values}
The motivation of Posterior predictive p-values(PPP) is similar to frequentist p-values. The PPP are however averaged over the entire posterior distribution of the test statistic. Given a test statistic $T(\boldsymbol{y})$, the frequentist p-value is equal to the probability $P(T(\boldsymbol{\tilde{y}}) > T(\boldsymbol{y}))$, where $p(\boldsymbol{\tilde{y}}) = p(\boldsymbol{y}|\boldsymbol{\hat{\theta}})$. In the Bayesian paradigm the parameter $\boldsymbol{\theta}$ has a posterior distribution. Thus, although the same probability for the test statistic is calculated as before, but it is averaged over the entire posterior distribution $p(\theta|\boldsymbol{y})$. A small PPP value indicates bad model fit. In context of the Bayesian heterogeneity model PPP values may not be as desirable for testing over/under fitted for the reasons mentioned in the previous subsection. However they can still be used to detect a poor fit to the data itself.
