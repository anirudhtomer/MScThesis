% !TEX root =  ../../thesis.tex

\chapter{Model selection criteria}
\label{ch : model_selection}

In most cases we do not know the right number of mixture components in advance unless we have some expert knowledge available or we know them from a previous/similar study. As part of this thesis we will compare many of the existing methods for finding the right number of mixture components.

\section{Deviance information criteria}
\label{sec : dic}

The Deviance information criteria or DIC was first proposed by \citet{spiegelhalter_bayesian_2002} for Bayesian model selection. The idea is similar to frequentist AIC/BIC criteria in the sense that DIC also penalizes more elaborate models using a penalty component. The definition for DIC is given by 

$$\text{DIC} = -2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})} + 2\text{p}_\text{D}$$
where $\boldsymbol{\bar{\theta}} = \text{E}(\boldsymbol{\theta}|\boldsymbol{y})$,\\
$\text{p}_\text{D} = -2\text{E}_{\boldsymbol{\theta}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\theta}|\boldsymbol{y}])}) + \log{p(\boldsymbol{y}|\boldsymbol{\bar{\theta}})})$ is the penalty for model complexity, and can also be written as 

$$\text{p}_\text{D}=\overline{D(\boldsymbol{\theta})} - D(\boldsymbol{\bar{\theta}})$$

where $D(\boldsymbol{\theta}) = -2\log{p(\boldsymbol{y}|[\boldsymbol{\theta}|\boldsymbol{y}])} + 2\log{f(\boldsymbol{y})}$ is called the Bayesian deviance. The term $f(\boldsymbol{y})$ however cancels out in the expression for $\text{p}_\text{D}$ and hence is not discussed.

\subsection{DIC for missing data models}
\label{subsec : DIC_missing_data_models}
Mixture models and mixed models both are both a member of the clas of models called missing data models. The reason is that the allocation vector $\boldsymbol{S}$ in a mixture model and matrix of random effects $\boldsymbol{b}=(\boldsymbol{b}_1, \boldsymbol{b}_1, ..., \boldsymbol{b}_n)$ in a LMM, both are not observed directly. Thus one could have various incompatible definitions of DIC based on observed data likelihood, complete data likelihood and conditional data likelihood, as shown by DeIorio and Robert in a discussion on the paper of \citet{spiegelhalter_bayesian_2002}. Further, \citet{celeux_deviance_2006} proposed multiple definitions of DIC under each of the aforementioned likelihood classes and showed that each has a different value and thus different impact on model selection. In this thesis we will take some of those definitions and apply them in context of the Bayesian heterogeneity model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Observed Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Observed DIC}
The first category of DIC's is associated with observed data likelihood $f(\boldsymbol{y}|\boldsymbol{\theta})$ or in our case $f(\boldsymbol{y}|\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu})$, where $\boldsymbol{\nu}$ is as defined in section \ref{subsec : bhtge}. The observed likelihood can be obtained by marginalizing over the allocation vector of subjects $\boldsymbol{S}$ and random effects $\boldsymbol{b}$. This give us the following formula for observed data likelihood.

\begin{equation}
\label{eq : obs_data_likelihood}
f(\boldsymbol{y}|\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}) = \prod_{i=1}^n \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_k^C, \boldsymbol{Z}_{i} G_k \boldsymbol{Z}_{i}^T+ R_i) \eta_k
\end{equation}
 
Based on equation \ref{eq : obs_data_likelihood} we will now extend the definition of $\text{DIC}_1$, $\text{DIC}_2$ and $\text{DIC}_3$ proposed by \citet{celeux_deviance_2006} to give

\begin{equation}
\label{eq : DIC1}
\text{DIC}_1 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}=\text{E}(\boldsymbol{\beta}|\boldsymbol{y})$, 
$\bar{\sigma^2}=\text{E}(\sigma^2|\boldsymbol{y})$ and 
$\boldsymbol{\bar{\nu}}=\text{E}(\boldsymbol{\nu}|\boldsymbol{y})$,\\

$\text{DIC}_2$'s definition is similar to $\text{DIC}_1$ but intead of posterior mean, posterior mode is used in calculation of $D(\boldsymbol{\bar{\theta}})$. It is given by

\begin{equation}
\label{eq : DIC2}
\text{DIC}_2 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\beta}}, \hat{\sigma^2}, \boldsymbol{\hat{\nu}})})
\end{equation}

where
$\boldsymbol{\hat{\beta}} = \argmax_{\boldsymbol{\beta}}p(\boldsymbol{\beta}|\boldsymbol{y})$,
$\hat{\sigma^2} = \argmax_{\sigma^2}p(\sigma^2|\boldsymbol{y})$ and
$\boldsymbol{\hat{\nu}} = \argmax_{\boldsymbol{\nu}}p(\boldsymbol{\nu}|\boldsymbol{y})$,\\

\citet{celeux_deviance_2006} suggest that for models where non identifiability of parameters is endemic, as is the case for mixtures usually, one should use an estimator $\hat{f}(\boldsymbol{y})$ for the approximation of the density $p(\boldsymbol{y} | \boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu})$. They further propose the following estimator for $\hat{f}(\boldsymbol{y})$ which uses posterior samples $\boldsymbol{\theta}^{(l)}$ from the $l^{\text{th}}$ MCMC iteration of a chain of length $m$.

$$\hat{f}(\boldsymbol{y}) = \prod_{i=1}^n \hat{f}(\boldsymbol{y}_i) = \prod_{i=1}^n \frac 1 m \sum_{l=1}^m \sum_{k=1}^K f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta}^{(l)} + \boldsymbol{Z}_i {\boldsymbol{b}_k^C}^{(l)}, \boldsymbol{Z}_{i} G_k^{(l)} \boldsymbol{Z}_{i}^T+ R_i^{(l)}) \eta_k^{(l)}$$

This gives us the following definition of DIC.

\begin{equation}
\label{eq : DIC3}
\text{DIC}_3 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) + 2\log{\hat{f}(\boldsymbol{y})}
\end{equation}

In each of the equations \ref{eq : DIC1}, \ref{eq : DIC2}, \ref{eq : DIC3}, the calculation of $\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])}) $ can be done by approximating it using the results from the MCMC iterations in the following way.

\begin{equation}
\label{eq : mean_posterior_deviance_approx}
\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}}(\log{p(\boldsymbol{y}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}])})  = \frac 1 m \sum_{l=1}^m\log{p(\boldsymbol{y}|\boldsymbol{\beta}^{(l)}, {\sigma^2}^{(l)}, \boldsymbol{\nu}^{(l)})})
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Complete Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Complete DIC}
The second class of the DIC is based on the complete data likelihood. Complete data for the $i^\text{th}$ subject in a Bayesian heterogeneity model will be $(\boldsymbol{y}_i, S_i, \boldsymbol{b}_i)$. The following equation shows the complete data likelihood of the data at hand.

\begin{equation}
\label{eq : complete_data_likelihood}
f(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S} | \boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}) = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i) f_N(\boldsymbol{b}_i; \boldsymbol{b}_{S_i}^C, G_{S_i}) \eta_{S_i}
\end{equation}

The formulation of complete data DIC is straightforward as we assume $(\boldsymbol{b}, \boldsymbol{S})$ to be observed. It can be written down as,

\begin{equation}
\label{eq : complete_data_dic}
\text{DIC} = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}=\text{E}(\boldsymbol{\beta}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$, 
$\bar{\sigma^2}=\text{E}(\sigma^2|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$ and 
$\boldsymbol{\bar{\nu}}=\text{E}(\boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S})$,\\

Unfortunately $(\boldsymbol{b}, \boldsymbol{S})$ are latent and thus \citet{celeux_deviance_2006} propose integrating the expression in \ref{eq : complete_data_dic} with respect to $(\boldsymbol{b}, \boldsymbol{S})$ to obtain the following definition of DIC.

\begin{equation}
\label{eq : DIC4}
\text{DIC}_4 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu},\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\text{E}_{\boldsymbol{b},\boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})
\end{equation}

where 
$\boldsymbol{\bar{\beta}}$, $\bar{\sigma^2}$ and $\boldsymbol{\bar{\nu}}$ remain same as for \ref{eq : complete_data_dic}. The first part of the formula for $\text{DIC}_4$ is not available in closed form for Bayesian heterogeneity model, however it can still be approximated using the output of Gibbs sampler in the same way as in \ref{eq : mean_posterior_deviance_approx}. However although one has to also simulate $(\boldsymbol{b}, \boldsymbol{S})$ in the MCMC iterations. The reason it works is that during each iteration of the Gibbs sampler, it simulates parameter values from the conditional distribution of the parameters. i.e. conditional on every other parameter being simulated in the chain, including the unobserved data. We further verified this approach by comparing the results of $\text{DIC}_4$ approximation for mixture distribution given by \citet{celeux_deviance_2006} with our approach and found the results to be differing only by a few decimal places.\\

The second part of $\text{DIC}_4$, i.e. $\text{E}_{\boldsymbol{b},\boldsymbol{S}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|\boldsymbol{\bar{\beta}}, \bar{\sigma^2}, \boldsymbol{\bar{\nu}})})$, is also not straightforward to compute. While the expectation over $\boldsymbol{b}, \boldsymbol{S}$ can be approximated in the same way as in \ref{eq : mean_posterior_deviance_approx} but for calculating $\boldsymbol{\bar{\beta}}$, $\bar{\sigma^2}$ and $\boldsymbol{\bar{\nu}}$, \citet{celeux_deviance_2006} suggest using the posterior estimates $(\boldsymbol{b}, \boldsymbol{S} | \boldsymbol{y})$ of the unobserved data. We will now give the formulae for the expected values of parameters of interest during the $l^\text{th}$ iteration $\boldsymbol{\bar{\beta}}^{(l)}$, $\bar{\sigma^2}^{(l)}$ and $\boldsymbol{\bar{\nu}}^{(l)}$.

$$\boldsymbol{\bar{\beta}}^{(l)} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{Zb^{(l)}})$$
$$\bar{\sigma^2}^{(l)} = \frac {(\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})^T (\boldsymbol{y}-\boldsymbol{Zb^{(l)}}-\boldsymbol{X}\boldsymbol{\bar{\beta}}^{(l)})} {(\sum_{i=1}^n m_i - p - 1) -2}$$
$$\bar{\boldsymbol{b}_k^C}^{(l)} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) \boldsymbol{b_i^{(l)}}} {n_k^{(l)}}$$
$$\bar{G_k^{(l)}} = \frac {\sum_{i=1}^n I(S_i^{(l)}=k) (\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})(\boldsymbol{b_i}^{(l)}-\bar{\boldsymbol{b}_k^C}^{(l)})^T} 
{(n_k^{(l)} - 1) - \text{rank}(\bar{\boldsymbol{b}_k^C}^{(l)}) - 1}$$
$$\bar{\eta}_k^{(l)} = \frac {a_k + n_k^{(l)}} {\sum_{u=1}^K a_u + n}$$

The next definition of DIC under the class of complete data DIC's is motivated by the fact that the at times $\text{E}(\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y})$ takes values outside the support of the joint distribution of $\boldsymbol{b}, \boldsymbol{S}$ \citet{celeux_deviance_2006}. Thus using MAP(maximum a posteriori) as the estimate instead, the following definition of DIC is proposed.

\begin{equation}
\label{eq : DIC5}
\text{DIC}_5 = -4\text{E}_{\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu},\boldsymbol{b}, \boldsymbol{S}|\boldsymbol{y}}(\log{p(\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}|[\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}|\boldsymbol{y}, \boldsymbol{b}, \boldsymbol{S}])}) + 
2\log{p(\boldsymbol{y}, \boldsymbol{\hat{b}}, \boldsymbol{\hat{S}}|\boldsymbol{\hat{\beta}}, \hat{\sigma^2}, \boldsymbol{\hat{\nu}})}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conditional Data DIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Conditional DIC}
The third class of the DIC is based on the assumption that missing data i.e. allocation vector $\boldsymbol{S}$ and random effects $\boldsymbol{b}_i$ can be seen as additional parameter rather than as missing data. We will represent the new posterior parameter space as $\boldsymbol{\theta}_\text{cond} = $($\boldsymbol{\beta}, \sigma^2, \boldsymbol{\nu}, \boldsymbol{S}, \boldsymbol{b}_1, \boldsymbol{b}_2, ..., \boldsymbol{b}_n)$. This leads to the conditional data likelihood,

\begin{equation}
\label{eq : conditional_data_likelihood}
f(\boldsymbol{y}|\boldsymbol{\theta}_\text{cond}) = \prod_{i=1}^n f_N(\boldsymbol{y}_i; \boldsymbol{X}_i\boldsymbol{\beta} + \boldsymbol{Z}_i \boldsymbol{b}_i, R_i)
\end{equation}

Based on this conditional likelihood, \citet{celeux_deviance_2006} proposed the following DIC definition.

\begin{equation}
\label{eq : DIC6}
\text{DIC}_6 = -4\text{E}_{\boldsymbol{\theta}_\text{cond}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\theta}_{\text{cond}}|\boldsymbol{y}])}) + 2\log{p(\boldsymbol{y}|\boldsymbol{\hat{\theta}}_\text{cond})})
\end{equation}

where
$\boldsymbol{\hat{\theta}}_\text{cond} = \argmax_{\boldsymbol{\theta}_\text{cond}}p(\boldsymbol{\theta}_\text{cond}|\boldsymbol{y})$, and $\text{E}_{\boldsymbol{\theta}_\text{cond}|\boldsymbol{y}} (\log{p(\boldsymbol{y}|[\boldsymbol{\theta}_{\text{cond}})}|\boldsymbol{y}])$ can be approximated as done in equation \ref{eq : mean_posterior_deviance_approx}.