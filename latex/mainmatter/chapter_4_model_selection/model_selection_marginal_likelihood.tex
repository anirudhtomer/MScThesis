
\section{Marginal Likelihood}
\label{sec : marginal_likelihood}

The marginal likelihood of data represents the probablilty of data given the model. This can be calculated by margilizing over the model parameters $\boldsymbol{\theta}$. 

\begin{equation}
\label{eq : marginal_likelihood}
p(\boldsymbol{y}|M) = \int p(\boldsymbol{y}|\boldsymbol{\theta}, M) p(\boldsymbol{\theta}|M) \diff \boldsymbol{\theta}
\end{equation}

Given two proposed models for the data, $M_1$ and $M_2$, one can further use the quantity in equation \ref{eq : marginal_likelihood} to calculate model evidence. The idea is to calculate the odds of model $M_1$ against the model $M_2$ given the data. i.e. Posterior odds. This ofcourse means that it is a comparative measure as $\sim M_1 = M_2$. One can write the posterior odds as

$$\frac {p(M_1|\boldsymbol{y})}{p(M_2|\boldsymbol{y})} = \frac {p(\boldsymbol{y}|M_1) p(M_1)} {p(\boldsymbol{y}|M_2) p(M_2)}$$

where $\frac {p(M_1)}{p(M_2)}$ is called prior odds, and $\frac {p(\boldsymbol{y}|M_1)} {p(\boldsymbol{y}|M_2)}$ is called the Bayes Factor.\\

Since we have the same belief in each of these models, prior odds is equal to 1 apriori. To calculate the Bayes Factor we will use the method proposed by \citet{chib_marginal_1995}. Chib's idea is that one can rewrite the Bayes rule in equation \ref{eq : bayes_rule} to get the marginal likelihood formula as

\begin{equation}
\label{eq : chib_trick}
m(\boldsymbol{y}) = p(\boldsymbol{y}|M) = \dfrac {L(\boldsymbol{\theta}|\boldsymbol{y}, M) p(\boldsymbol{\theta}|M)} {p(\boldsymbol{\theta}|\boldsymbol{y}, M)}
\end{equation}

Equation \ref{eq : chib_trick} is valid for all $\boldsymbol{\theta}$, though Chib recommends using posterior mode $\argmax_\theta p(\boldsymbol{\theta}|\boldsymbol{y})$. Let's denote the selected $\boldsymbol{\theta}$ as $\boldsymbol{\theta}^*$. Chib's approximation for marginal likelihood on log scale is given by,

\begin{equation}
\label{eq : chib_approx}
\log{\hat{m}(\boldsymbol{y})} = \log{L(\boldsymbol{\theta}^*|\boldsymbol{y})} + \log{p(\boldsymbol{\theta}^*)} - \log{p(\boldsymbol{\theta}^*|\boldsymbol{y})}
\end{equation}

Note that we have dropped the model indicator $M$ from equation \ref{eq : chib_approx} for readability in further equations. We will now further solve this equation using the parameters from the Bayesian heterogeneity model discussed in the previous chapter. 
An interesting aspect of this equation is that if the posterior is not available in closed form then calculation of Marginal likelihood can become tedious. 

\section{Posterior predictive checks}
\label{sec : ppc}

The idea of the posterior predictive checks is to evaluate the model fit using simulations from the posterior predictive distribution(PPD) $p(\boldsymbol{\tilde{y}}|\boldsymbol{y}$. As an informal check one could sample 1000 values from the PPD 20 times and make 20 histograms to show the density. If the histograms do not match with the histogram of the original sample one could say that the model did not fit the data well.\\ 
A formal way to do this is using Posterior predictive p-values(PPP) or Bayesian p-values. In the frequentist paradigm after fitting a model based on parameter $\hat{\theta}$ one could test the model using test statistic. Let us represent that test statistic value for original sample to be $T(\boldsymbol{y})$. Now based on the sampling distribution of $T(\boldsymbol{\tilde{y}})$ we could check the probability $P(T(\boldsymbol{\tilde{y}}) > T(\boldsymbol{y})$. In the bayesian paradigm the parameter $\theta$ has a posterior distribution and so we find the same probability like before albeit averaged over the entire posterior $p(\theta|\boldsymbol{y})$. A small PPP value indicates bad fit of model to the data. For a complete interpretation of this p-value we refer the readers to \citet{gelman_understanding_2012}.
